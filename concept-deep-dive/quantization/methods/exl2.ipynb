{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EXL2\n",
        "A more recent format based on the GPTQ optimization method but with mixed quantization levels. It achieves an average desired bitrate with lower errors than GPTQ while keeping the same or similar bitrate. Can have a slightly higher VRAM usage but better inference speed and quality.\n",
        "### Quantizing with [exllamav2](https://github.com/turboderp/exllamav2)\n",
        "\n",
        "Lets do a short demo and quantize Mistral 7B!\n",
        "\n",
        "let's install `exllamav2` and all dependencies required."
      ],
      "metadata": {
        "id": "c0xs9Rd6bU9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/turboderp/exllamav2\n",
        "!(cd exllamav2 && pip install -r requirements.txt && pip install .)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60LFZishVYAx",
        "outputId": "ef8c20e2-cbb3-40ca-e0bf-b5376fa539e4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exllamav2'...\n",
            "remote: Enumerating objects: 6958, done.\u001b[K\n",
            "remote: Counting objects: 100% (1276/1276), done.\u001b[K\n",
            "remote: Compressing objects: 100% (550/550), done.\u001b[K\n",
            "remote: Total 6958 (delta 909), reused 1042 (delta 721), pack-reused 5682 (from 1)\u001b[K\n",
            "Receiving objects: 100% (6958/6958), 19.88 MiB | 24.76 MiB/s, done.\n",
            "Resolving deltas: 100% (4908/4908), done.\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.4)\n",
            "Collecting ninja (from -r requirements.txt (line 2))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.44.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (71.0.4)\n",
            "Collecting fastparquet (from -r requirements.txt (line 5))\n",
            "  Downloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.3.1+cu121)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.4.4)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.1.99)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.16.1)\n",
            "Collecting websockets (from -r requirements.txt (line 10))\n",
            "  Downloading websockets-13.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2024.5.15)\n",
            "Requirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (1.26.4)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (0.19.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (13.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 1)) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 1)) (2024.1)\n",
            "Collecting cramjam>=2.3 (from fastparquet->-r requirements.txt (line 5))\n",
            "  Downloading cramjam-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet->-r requirements.txt (line 5)) (2024.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet->-r requirements.txt (line 5)) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->-r requirements.txt (line 13)) (0.23.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->-r requirements.txt (line 14)) (3.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (4.66.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 14)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.2.0->-r requirements.txt (line 6)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.2.0->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (2024.7.4)\n",
            "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading websockets-13.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m321.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cramjam-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: ninja, websockets, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, cramjam, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, fastparquet\n",
            "Successfully installed cramjam-2.8.3 fastparquet-2024.5.0 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 websockets-13.0\n",
            "Processing /content/exllamav2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from exllamav2==0.1.8) (2.1.4)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from exllamav2==0.1.8) (1.11.1.1)\n",
            "Requirement already satisfied: fastparquet in /usr/local/lib/python3.10/dist-packages (from exllamav2==0.1.8) (2024.5.0)\n",
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from exllamav2==0.1.8) (2.3.1+cu121)\n",
            "Requirement already satisfied: safetensors>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from exllamav2==0.1.8) (0.4.4)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.10/dist-packages (from exllamav2==0.1.8) (0.1.99)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from exllamav2==0.1.8) (2.16.1)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.10/dist-packages (from exllamav2==0.1.8) (13.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from exllamav2==0.1.8) (2024.5.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from exllamav2==0.1.8) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from exllamav2==0.1.8) (13.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->exllamav2==0.1.8) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0->exllamav2==0.1.8) (12.6.20)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.10/dist-packages (from fastparquet->exllamav2==0.1.8) (2.8.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet->exllamav2==0.1.8) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->exllamav2==0.1.8) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->exllamav2==0.1.8) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->exllamav2==0.1.8) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->exllamav2==0.1.8) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->exllamav2==0.1.8) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->exllamav2==0.1.8) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.2.0->exllamav2==0.1.8) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.2.0->exllamav2==0.1.8) (1.3.0)\n",
            "Building wheels for collected packages: exllamav2\n",
            "  Building wheel for exllamav2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for exllamav2: filename=exllamav2-0.1.8-cp310-cp310-linux_x86_64.whl size=51731505 sha256=d0a886225c9d25596867e0c74a704666e260373813d207e0467a7ee71572b020\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mgnjedvg/wheels/5a/f8/dc/09b6466fc29d3ca538340226ed551df529f09a59f7650f5ec0\n",
            "Successfully built exllamav2\n",
            "Installing collected packages: exllamav2\n",
            "Successfully installed exllamav2-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once everything installed we can download the model."
      ],
      "metadata": {
        "id": "d-RUkKoPLMhZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuyfAPBiVMuL",
        "outputId": "f8b9efc0-5fd2-4e12-b2e2-f0d59fbf37b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'Mistral-7B-Instruct-v0.3'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 93 (delta 46), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (93/93), 734.97 KiB | 4.20 MiB/s, done.\n",
            "Filtering content: 100% (5/5), 3.00 GiB | 12.59 MiB/s, done.\n",
            "Encountered 4 file(s) that may not have been copied correctly on Windows:\n",
            "\tmodel-00002-of-00003.safetensors\n",
            "\tmodel-00003-of-00003.safetensors\n",
            "\tmodel-00001-of-00003.safetensors\n",
            "\tconsolidated.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ],
      "source": [
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "user_name = \"huggingface_username\"\n",
        "hf_token = \"read_token\"\n",
        "\n",
        "!git lfs install\n",
        "!git clone https://{user_name}:{hf_token}@huggingface.co/{model_id}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to quantize! Lets go with a bitrate of 4.0"
      ],
      "metadata": {
        "id": "6M9fdFAeLVUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = model_id.split('/')[-1]\n",
        "quant_bpw = 4.0\n",
        "\n",
        "!mkdir temp\n",
        "!python exllamav2/convert.py \\\n",
        "    -i {model_name} \\\n",
        "    -o temp/ \\\n",
        "    -cf {model_name}-exl2/{quant_bpw}bpw/ \\\n",
        "    -b {quant_bpw}"
      ],
      "metadata": {
        "id": "fVMcWYgxQHts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9794b33f-cf8e-423e-a3e1-9e29a1c95863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " -- model.layers.2.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.2.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.2.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.98242324\n",
            " -- 2.1987 bpw  accuracy: 0.98377117\n",
            " -- 2.2831 bpw  accuracy: 0.98568281\n",
            " -- 2.6768 bpw  accuracy: 0.98892056\n",
            " -- 3.1689 bpw  accuracy: 0.99101379\n",
            " -- 3.1705 bpw  accuracy: 0.99098715\n",
            " -- 4.0439 bpw  accuracy: 0.99453021\n",
            " -- 4.0471 bpw  accuracy: 0.99469697\n",
            " -- 4.0816 bpw  accuracy: 0.99504670\n",
            " -- 4.1381 bpw  accuracy: 0.99529099\n",
            " -- 4.1705 bpw  accuracy: 0.99531599\n",
            " -- 4.1902 bpw  accuracy: 0.99563936\n",
            " -- 4.2737 bpw  accuracy: 0.99624683\n",
            " -- 4.3295 bpw  accuracy: 0.99665906\n",
            " -- 5.2564 bpw  accuracy: 0.99801150\n",
            " -- 5.3295 bpw  accuracy: 0.99832702\n",
            " -- 6.0439 bpw  accuracy: 0.99850651\n",
            " -- 6.3381 bpw  accuracy: 0.99915358\n",
            " -- 8.0439 bpw  accuracy: 0.99959020\n",
            "--------------------------------------------\n",
            "| Measured: model.layers.2 (Attention)     |\n",
            "| Duration: 12.13 seconds                  |\n",
            "| Completed step: 5/67                     |\n",
            "| Avg time / step (rolling): 21.26 seconds |\n",
            "| Estimated remaining time: 21min 58sec    |\n",
            "| Last checkpoint layer: None              |\n",
            "--------------------------------------------\n",
            " -- Layer: model.layers.2 (MLP)\n",
            " -- model.layers.2.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.2.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.2.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.2.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.2.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.2.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.2.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.2.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.97782768\n",
            " -- 2.3230 bpw  accuracy: 0.97843042\n",
            " -- 2.5958 bpw  accuracy: 0.98211724\n",
            " -- 2.9120 bpw  accuracy: 0.98331300\n",
            " -- 3.2833 bpw  accuracy: 0.98874731\n",
            " -- 3.3655 bpw  accuracy: 0.98963368\n",
            " -- 3.6186 bpw  accuracy: 0.99129309\n",
            " -- 4.1368 bpw  accuracy: 0.99411108\n",
            " -- 4.1977 bpw  accuracy: 0.99466143\n",
            " -- 4.2662 bpw  accuracy: 0.99425921\n",
            " -- 4.3484 bpw  accuracy: 0.99497052\n",
            " -- 5.2491 bpw  accuracy: 0.99707504\n",
            " -- 5.3313 bpw  accuracy: 0.99749370\n",
            " -- 6.0713 bpw  accuracy: 0.99841222\n",
            " -- 6.3032 bpw  accuracy: 0.99850878\n",
            " -- 6.8687 bpw  accuracy: 0.99884521\n",
            " -- 8.0354 bpw  accuracy: 0.99957238\n",
            "--------------------------------------------\n",
            "| Measured: model.layers.2 (MLP)           |\n",
            "| Duration: 33.99 seconds                  |\n",
            "| Completed step: 6/67                     |\n",
            "| Avg time / step (rolling): 23.38 seconds |\n",
            "| Estimated remaining time: 23min 46sec    |\n",
            "| Last checkpoint layer: None              |\n",
            "--------------------------------------------\n",
            " -- Layer: model.layers.3 (Attention)\n",
            " -- model.layers.3.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.3.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.3.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.3.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.3.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.98109958\n",
            " -- 2.1987 bpw  accuracy: 0.98228964\n",
            " -- 2.2831 bpw  accuracy: 0.98462530\n",
            " -- 2.6768 bpw  accuracy: 0.98846251\n",
            " -- 3.1689 bpw  accuracy: 0.98989627\n",
            " -- 3.1705 bpw  accuracy: 0.99019452\n",
            " -- 4.0439 bpw  accuracy: 0.99267435\n",
            " -- 4.0471 bpw  accuracy: 0.99309814\n",
            " -- 4.0816 bpw  accuracy: 0.99372198\n",
            " -- 4.1381 bpw  accuracy: 0.99405704\n",
            " -- 4.1705 bpw  accuracy: 0.99500043\n",
            " -- 4.1902 bpw  accuracy: 0.99540469\n",
            " -- 4.2737 bpw  accuracy: 0.99572001\n",
            " -- 4.3295 bpw  accuracy: 0.99607138\n",
            " -- 5.2564 bpw  accuracy: 0.99757583\n",
            " -- 5.3295 bpw  accuracy: 0.99799545\n",
            " -- 6.0439 bpw  accuracy: 0.99800412\n",
            " -- 6.3381 bpw  accuracy: 0.99908931\n",
            " -- 8.0439 bpw  accuracy: 0.99947425\n",
            "--------------------------------------------\n",
            "| Measured: model.layers.3 (Attention)     |\n",
            "| Duration: 11.98 seconds                  |\n",
            "| Completed step: 7/67                     |\n",
            "| Avg time / step (rolling): 21.75 seconds |\n",
            "| Estimated remaining time: 21min 45sec    |\n",
            "| Last checkpoint layer: None              |\n",
            "--------------------------------------------\n",
            " -- Layer: model.layers.3 (MLP)\n",
            " -- model.layers.3.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.3.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.3.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.3.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.3.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.3.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.3.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.3.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.97330996\n",
            " -- 2.3230 bpw  accuracy: 0.97404764\n",
            " -- 2.5958 bpw  accuracy: 0.97811245\n",
            " -- 2.9120 bpw  accuracy: 0.97940267\n",
            " -- 3.2833 bpw  accuracy: 0.98638627\n",
            " -- 3.3655 bpw  accuracy: 0.98746937\n",
            " -- 3.6186 bpw  accuracy: 0.98926819\n",
            " -- 4.1368 bpw  accuracy: 0.99289792\n",
            " -- 4.1977 bpw  accuracy: 0.99355503\n",
            " -- 4.2662 bpw  accuracy: 0.99305358\n",
            " -- 4.3484 bpw  accuracy: 0.99391918\n",
            " -- 5.2491 bpw  accuracy: 0.99645910\n",
            " -- 5.3313 bpw  accuracy: 0.99696968\n",
            " -- 6.0713 bpw  accuracy: 0.99808397\n",
            " -- 6.3032 bpw  accuracy: 0.99819815\n",
            " -- 6.8687 bpw  accuracy: 0.99855809\n",
            " -- 8.0354 bpw  accuracy: 0.99948509\n",
            "--------------------------------------------\n",
            "| Measured: model.layers.3 (MLP)           |\n",
            "| Duration: 33.74 seconds                  |\n",
            "| Completed step: 8/67                     |\n",
            "| Avg time / step (rolling): 23.25 seconds |\n",
            "| Estimated remaining time: 22min 51sec    |\n",
            "| Last checkpoint layer: None              |\n",
            "--------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.4 (Attention)\n",
            " -- model.layers.4.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.4.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.4.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.4.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.4.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.98252585\n",
            " -- 2.1987 bpw  accuracy: 0.98348221\n",
            " -- 2.2831 bpw  accuracy: 0.98561841\n",
            " -- 2.6768 bpw  accuracy: 0.98871386\n",
            " -- 3.1689 bpw  accuracy: 0.99040757\n",
            " -- 3.1705 bpw  accuracy: 0.99075949\n",
            " -- 4.0439 bpw  accuracy: 0.99289451\n",
            " -- 4.0471 bpw  accuracy: 0.99318537\n",
            " -- 4.0816 bpw  accuracy: 0.99409265\n",
            " -- 4.1381 bpw  accuracy: 0.99428539\n",
            " -- 4.1705 bpw  accuracy: 0.99511318\n",
            " -- 4.1902 bpw  accuracy: 0.99563079\n",
            " -- 4.2737 bpw  accuracy: 0.99580912\n",
            " -- 4.3295 bpw  accuracy: 0.99628595\n",
            " -- 5.2564 bpw  accuracy: 0.99768850\n",
            " -- 5.3295 bpw  accuracy: 0.99816330\n",
            " -- 6.0439 bpw  accuracy: 0.99805899\n",
            " -- 6.3381 bpw  accuracy: 0.99915348\n",
            " -- 8.0439 bpw  accuracy: 0.99948567\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.4 (Attention)        |\n",
            "| Duration: 11.79 seconds                     |\n",
            "| Completed step: 9/67                        |\n",
            "| Avg time / step (rolling): 21.98 seconds    |\n",
            "| Estimated remaining time: 21min 14sec       |\n",
            "| Last checkpoint layer: model.layers.3 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.4 (MLP)\n",
            " -- model.layers.4.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.4.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.4.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.4.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.4.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.4.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.4.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.4.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.96977647\n",
            " -- 2.3230 bpw  accuracy: 0.97067890\n",
            " -- 2.5958 bpw  accuracy: 0.97548738\n",
            " -- 2.9120 bpw  accuracy: 0.97696493\n",
            " -- 3.2833 bpw  accuracy: 0.98466234\n",
            " -- 3.3655 bpw  accuracy: 0.98591853\n",
            " -- 3.6186 bpw  accuracy: 0.98797810\n",
            " -- 4.1368 bpw  accuracy: 0.99197283\n",
            " -- 4.1977 bpw  accuracy: 0.99271050\n",
            " -- 4.2662 bpw  accuracy: 0.99217591\n",
            " -- 4.3484 bpw  accuracy: 0.99316832\n",
            " -- 5.2491 bpw  accuracy: 0.99601106\n",
            " -- 5.3313 bpw  accuracy: 0.99659400\n",
            " -- 6.0713 bpw  accuracy: 0.99782196\n",
            " -- 6.3032 bpw  accuracy: 0.99796597\n",
            " -- 6.8687 bpw  accuracy: 0.99838741\n",
            " -- 8.0354 bpw  accuracy: 0.99940790\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.4 (MLP)              |\n",
            "| Duration: 33.87 seconds                     |\n",
            "| Completed step: 10/67                       |\n",
            "| Avg time / step (rolling): 23.17 seconds    |\n",
            "| Estimated remaining time: 22min 0sec        |\n",
            "| Last checkpoint layer: model.layers.3 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.5 (Attention)\n",
            " -- model.layers.5.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.5.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.5.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.5.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.5.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.97664799\n",
            " -- 2.1987 bpw  accuracy: 0.97792282\n",
            " -- 2.2831 bpw  accuracy: 0.98103946\n",
            " -- 2.6768 bpw  accuracy: 0.98568327\n",
            " -- 3.1689 bpw  accuracy: 0.98804598\n",
            " -- 3.1705 bpw  accuracy: 0.98837588\n",
            " -- 4.0439 bpw  accuracy: 0.99172002\n",
            " -- 4.0471 bpw  accuracy: 0.99216411\n",
            " -- 4.0816 bpw  accuracy: 0.99320019\n",
            " -- 4.1381 bpw  accuracy: 0.99348132\n",
            " -- 4.1705 bpw  accuracy: 0.99413244\n",
            " -- 4.1902 bpw  accuracy: 0.99458293\n",
            " -- 4.2737 bpw  accuracy: 0.99511615\n",
            " -- 4.3295 bpw  accuracy: 0.99549039\n",
            " -- 5.2564 bpw  accuracy: 0.99733608\n",
            " -- 5.3295 bpw  accuracy: 0.99771802\n",
            " -- 6.0439 bpw  accuracy: 0.99795961\n",
            " -- 6.3381 bpw  accuracy: 0.99883434\n",
            " -- 8.0439 bpw  accuracy: 0.99945571\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.5 (Attention)        |\n",
            "| Duration: 11.86 seconds                     |\n",
            "| Completed step: 11/67                       |\n",
            "| Avg time / step (rolling): 22.94 seconds    |\n",
            "| Estimated remaining time: 21min 24sec       |\n",
            "| Last checkpoint layer: model.layers.3 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.5 (MLP)\n",
            " -- model.layers.5.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.5.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.5.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.5.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.5.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.5.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.5.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.5.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.96435021\n",
            " -- 2.3230 bpw  accuracy: 0.96540036\n",
            " -- 2.5958 bpw  accuracy: 0.97121513\n",
            " -- 2.9120 bpw  accuracy: 0.97297229\n",
            " -- 3.2833 bpw  accuracy: 0.98198438\n",
            " -- 3.3655 bpw  accuracy: 0.98342225\n",
            " -- 3.6186 bpw  accuracy: 0.98589354\n",
            " -- 4.1368 bpw  accuracy: 0.99059182\n",
            " -- 4.1977 bpw  accuracy: 0.99144911\n",
            " -- 4.2662 bpw  accuracy: 0.99083530\n",
            " -- 4.3484 bpw  accuracy: 0.99197662\n",
            " -- 5.2491 bpw  accuracy: 0.99533982\n",
            " -- 5.3313 bpw  accuracy: 0.99600649\n",
            " -- 6.0713 bpw  accuracy: 0.99746123\n",
            " -- 6.3032 bpw  accuracy: 0.99763201\n",
            " -- 6.8687 bpw  accuracy: 0.99813379\n",
            " -- 8.0354 bpw  accuracy: 0.99931030\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.5 (MLP)              |\n",
            "| Duration: 33.92 seconds                     |\n",
            "| Completed step: 12/67                       |\n",
            "| Avg time / step (rolling): 22.94 seconds    |\n",
            "| Estimated remaining time: 21min 1sec        |\n",
            "| Last checkpoint layer: model.layers.3 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.6 (Attention)\n",
            " -- model.layers.6.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.6.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.6.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.6.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.6.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.97556552\n",
            " -- 2.1987 bpw  accuracy: 0.97681589\n",
            " -- 2.2831 bpw  accuracy: 0.97946853\n",
            " -- 2.6768 bpw  accuracy: 0.98439448\n",
            " -- 3.1689 bpw  accuracy: 0.98764152\n",
            " -- 3.1705 bpw  accuracy: 0.98793082\n",
            " -- 4.0439 bpw  accuracy: 0.99126788\n",
            " -- 4.0471 bpw  accuracy: 0.99168501\n",
            " -- 4.0816 bpw  accuracy: 0.99258404\n",
            " -- 4.1381 bpw  accuracy: 0.99307442\n",
            " -- 4.1705 bpw  accuracy: 0.99386278\n",
            " -- 4.1902 bpw  accuracy: 0.99437859\n",
            " -- 4.2737 bpw  accuracy: 0.99466031\n",
            " -- 4.3295 bpw  accuracy: 0.99514121\n",
            " -- 5.2564 bpw  accuracy: 0.99706046\n",
            " -- 5.3295 bpw  accuracy: 0.99758078\n",
            " -- 6.0439 bpw  accuracy: 0.99770153\n",
            " -- 6.3381 bpw  accuracy: 0.99877531\n",
            " -- 8.0439 bpw  accuracy: 0.99936667\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.6 (Attention)        |\n",
            "| Duration: 11.83 seconds                     |\n",
            "| Completed step: 13/67                       |\n",
            "| Avg time / step (rolling): 22.89 seconds    |\n",
            "| Estimated remaining time: 20min 35sec       |\n",
            "| Last checkpoint layer: model.layers.3 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.6 (MLP)\n",
            " -- model.layers.6.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.6.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.6.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.6.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.6.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.6.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.6.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.6.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.96055157\n",
            " -- 2.3230 bpw  accuracy: 0.96168662\n",
            " -- 2.5958 bpw  accuracy: 0.96828757\n",
            " -- 2.9120 bpw  accuracy: 0.97028112\n",
            " -- 3.2833 bpw  accuracy: 0.98004671\n",
            " -- 3.3655 bpw  accuracy: 0.98168241\n",
            " -- 3.6186 bpw  accuracy: 0.98446003\n",
            " -- 4.1368 bpw  accuracy: 0.98954397\n",
            " -- 4.1977 bpw  accuracy: 0.99051610\n",
            " -- 4.2662 bpw  accuracy: 0.98983606\n",
            " -- 4.3484 bpw  accuracy: 0.99112281\n",
            " -- 5.2491 bpw  accuracy: 0.99482468\n",
            " -- 5.3313 bpw  accuracy: 0.99557457\n",
            " -- 6.0713 bpw  accuracy: 0.99716802\n",
            " -- 6.3032 bpw  accuracy: 0.99736536\n",
            " -- 6.8687 bpw  accuracy: 0.99794299\n",
            " -- 8.0354 bpw  accuracy: 0.99923152\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.6 (MLP)              |\n",
            "| Duration: 33.80 seconds                     |\n",
            "| Completed step: 14/67                       |\n",
            "| Avg time / step (rolling): 22.89 seconds    |\n",
            "| Estimated remaining time: 20min 13sec       |\n",
            "| Last checkpoint layer: model.layers.3 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.7 (Attention)\n",
            " -- model.layers.7.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.7.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.7.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.7.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.7.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.97214290\n",
            " -- 2.1987 bpw  accuracy: 0.97319920\n",
            " -- 2.2831 bpw  accuracy: 0.97652968\n",
            " -- 2.6768 bpw  accuracy: 0.98208666\n",
            " -- 3.1689 bpw  accuracy: 0.98597534\n",
            " -- 3.1705 bpw  accuracy: 0.98647040\n",
            " -- 4.0439 bpw  accuracy: 0.99031376\n",
            " -- 4.0471 bpw  accuracy: 0.99082901\n",
            " -- 4.0816 bpw  accuracy: 0.99154732\n",
            " -- 4.1381 bpw  accuracy: 0.99214164\n",
            " -- 4.1705 bpw  accuracy: 0.99316714\n",
            " -- 4.1902 bpw  accuracy: 0.99356058\n",
            " -- 4.2737 bpw  accuracy: 0.99389419\n",
            " -- 4.3295 bpw  accuracy: 0.99452353\n",
            " -- 5.2564 bpw  accuracy: 0.99661940\n",
            " -- 5.3295 bpw  accuracy: 0.99726697\n",
            " -- 6.0439 bpw  accuracy: 0.99739566\n",
            " -- 6.3381 bpw  accuracy: 0.99846183\n",
            " -- 8.0439 bpw  accuracy: 0.99932787\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.7 (Attention)        |\n",
            "| Duration: 11.93 seconds                     |\n",
            "| Completed step: 15/67                       |\n",
            "| Avg time / step (rolling): 22.87 seconds    |\n",
            "| Estimated remaining time: 19min 49sec       |\n",
            "| Last checkpoint layer: model.layers.3 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.7 (MLP)\n",
            " -- model.layers.7.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.7.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.7.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.7.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.7.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.7.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.7.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.7.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.95711282\n",
            " -- 2.3230 bpw  accuracy: 0.95840440\n",
            " -- 2.5958 bpw  accuracy: 0.96528786\n",
            " -- 2.9120 bpw  accuracy: 0.96737304\n",
            " -- 3.2833 bpw  accuracy: 0.97833324\n",
            " -- 3.3655 bpw  accuracy: 0.98013736\n",
            " -- 3.6186 bpw  accuracy: 0.98302650\n",
            " -- 4.1368 bpw  accuracy: 0.98867399\n",
            " -- 4.1977 bpw  accuracy: 0.98971790\n",
            " -- 4.2662 bpw  accuracy: 0.98894402\n",
            " -- 4.3484 bpw  accuracy: 0.99036205\n",
            " -- 5.2491 bpw  accuracy: 0.99436124\n",
            " -- 5.3313 bpw  accuracy: 0.99519460\n",
            " -- 6.0713 bpw  accuracy: 0.99691936\n",
            " -- 6.3032 bpw  accuracy: 0.99712549\n",
            " -- 6.8687 bpw  accuracy: 0.99772370\n",
            " -- 8.0354 bpw  accuracy: 0.99917376\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.7 (MLP)              |\n",
            "| Duration: 33.84 seconds                     |\n",
            "| Completed step: 16/67                       |\n",
            "| Avg time / step (rolling): 22.86 seconds    |\n",
            "| Estimated remaining time: 19min 25sec       |\n",
            "| Last checkpoint layer: model.layers.3 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.8 (Attention)\n",
            " -- model.layers.8.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.8.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.8.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.8.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.8.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.97076562\n",
            " -- 2.1987 bpw  accuracy: 0.97229849\n",
            " -- 2.2831 bpw  accuracy: 0.97526232\n",
            " -- 2.6768 bpw  accuracy: 0.98203508\n",
            " -- 3.1689 bpw  accuracy: 0.98482448\n",
            " -- 3.1705 bpw  accuracy: 0.98501838\n",
            " -- 4.0439 bpw  accuracy: 0.98959962\n",
            " -- 4.0471 bpw  accuracy: 0.98982176\n",
            " -- 4.0816 bpw  accuracy: 0.99111000\n",
            " -- 4.1381 bpw  accuracy: 0.99150279\n",
            " -- 4.1705 bpw  accuracy: 0.99236452\n",
            " -- 4.1902 bpw  accuracy: 0.99303760\n",
            " -- 4.2737 bpw  accuracy: 0.99337368\n",
            " -- 4.3295 bpw  accuracy: 0.99398298\n",
            " -- 5.2564 bpw  accuracy: 0.99635961\n",
            " -- 5.3295 bpw  accuracy: 0.99702405\n",
            " -- 6.0439 bpw  accuracy: 0.99729517\n",
            " -- 6.3381 bpw  accuracy: 0.99846129\n",
            " -- 8.0439 bpw  accuracy: 0.99928982\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.8 (Attention)        |\n",
            "| Duration: 11.84 seconds                     |\n",
            "| Completed step: 17/67                       |\n",
            "| Avg time / step (rolling): 22.84 seconds    |\n",
            "| Estimated remaining time: 19min 2sec        |\n",
            "| Last checkpoint layer: model.layers.7 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.8 (MLP)\n",
            " -- model.layers.8.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.8.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.8.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.8.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.8.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.8.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.8.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.8.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.95471036\n",
            " -- 2.3230 bpw  accuracy: 0.95603851\n",
            " -- 2.5958 bpw  accuracy: 0.96329175\n",
            " -- 2.9120 bpw  accuracy: 0.96548579\n",
            " -- 3.2833 bpw  accuracy: 0.97712556\n",
            " -- 3.3655 bpw  accuracy: 0.97902408\n",
            " -- 3.6186 bpw  accuracy: 0.98206723\n",
            " -- 4.1368 bpw  accuracy: 0.98805704\n",
            " -- 4.1977 bpw  accuracy: 0.98914909\n",
            " -- 4.2662 bpw  accuracy: 0.98834675\n",
            " -- 4.3484 bpw  accuracy: 0.98983174\n",
            " -- 5.2491 bpw  accuracy: 0.99406063\n",
            " -- 5.3313 bpw  accuracy: 0.99493112\n",
            " -- 6.0713 bpw  accuracy: 0.99675670\n",
            " -- 6.3032 bpw  accuracy: 0.99697408\n",
            " -- 6.8687 bpw  accuracy: 0.99760878\n",
            " -- 8.0354 bpw  accuracy: 0.99911201\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.8 (MLP)              |\n",
            "| Duration: 33.78 seconds                     |\n",
            "| Completed step: 18/67                       |\n",
            "| Avg time / step (rolling): 22.85 seconds    |\n",
            "| Estimated remaining time: 18min 39sec       |\n",
            "| Last checkpoint layer: model.layers.7 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.9 (Attention)\n",
            " -- model.layers.9.self_attn.q_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.9.self_attn.q_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.9.self_attn.k_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.9.self_attn.v_proj                    1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.9.self_attn.o_proj                    1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.96949360\n",
            " -- 2.1987 bpw  accuracy: 0.97062068\n",
            " -- 2.2831 bpw  accuracy: 0.97313075\n",
            " -- 2.6768 bpw  accuracy: 0.97934811\n",
            " -- 3.1689 bpw  accuracy: 0.98462158\n",
            " -- 3.1705 bpw  accuracy: 0.98493923\n",
            " -- 4.0439 bpw  accuracy: 0.98978280\n",
            " -- 4.0471 bpw  accuracy: 0.99026466\n",
            " -- 4.0816 bpw  accuracy: 0.99111835\n",
            " -- 4.1381 bpw  accuracy: 0.99121882\n",
            " -- 4.1705 bpw  accuracy: 0.99238511\n",
            " -- 4.1902 bpw  accuracy: 0.99289908\n",
            " -- 4.2737 bpw  accuracy: 0.99330442\n",
            " -- 4.3295 bpw  accuracy: 0.99388537\n",
            " -- 5.2564 bpw  accuracy: 0.99638738\n",
            " -- 5.3295 bpw  accuracy: 0.99697100\n",
            " -- 6.0439 bpw  accuracy: 0.99746045\n",
            " -- 6.3381 bpw  accuracy: 0.99833393\n",
            " -- 8.0439 bpw  accuracy: 0.99929864\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.9 (Attention)        |\n",
            "| Duration: 11.99 seconds                     |\n",
            "| Completed step: 19/67                       |\n",
            "| Avg time / step (rolling): 22.87 seconds    |\n",
            "| Estimated remaining time: 18min 17sec       |\n",
            "| Last checkpoint layer: model.layers.7 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.9 (MLP)\n",
            " -- model.layers.9.mlp.gate_proj                       0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.9.mlp.gate_proj                       1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.9.mlp.up_proj                         1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.9.mlp.up_proj                         1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.9.mlp.up_proj                         0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.9.mlp.up_proj                         1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.9.mlp.down_proj                       0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.9.mlp.down_proj                       1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.95229753\n",
            " -- 2.3230 bpw  accuracy: 0.95371278\n",
            " -- 2.5958 bpw  accuracy: 0.96133240\n",
            " -- 2.9120 bpw  accuracy: 0.96366655\n",
            " -- 3.2833 bpw  accuracy: 0.97580603\n",
            " -- 3.3655 bpw  accuracy: 0.97784011\n",
            " -- 3.6186 bpw  accuracy: 0.98109499\n",
            " -- 4.1368 bpw  accuracy: 0.98736480\n",
            " -- 4.1977 bpw  accuracy: 0.98850414\n",
            " -- 4.2662 bpw  accuracy: 0.98766696\n",
            " -- 4.3484 bpw  accuracy: 0.98923480\n",
            " -- 5.2491 bpw  accuracy: 0.99370057\n",
            " -- 5.3313 bpw  accuracy: 0.99464108\n",
            " -- 6.0713 bpw  accuracy: 0.99654098\n",
            " -- 6.3032 bpw  accuracy: 0.99678933\n",
            " -- 6.8687 bpw  accuracy: 0.99744834\n",
            " -- 8.0354 bpw  accuracy: 0.99905349\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.9 (MLP)              |\n",
            "| Duration: 33.90 seconds                     |\n",
            "| Completed step: 20/67                       |\n",
            "| Avg time / step (rolling): 22.87 seconds    |\n",
            "| Estimated remaining time: 17min 54sec       |\n",
            "| Last checkpoint layer: model.layers.7 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.10 (Attention)\n",
            " -- model.layers.10.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.10.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.10.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.10.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.10.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.96439050\n",
            " -- 2.1987 bpw  accuracy: 0.96588911\n",
            " -- 2.2831 bpw  accuracy: 0.96930288\n",
            " -- 2.6768 bpw  accuracy: 0.97711419\n",
            " -- 3.1689 bpw  accuracy: 0.98197677\n",
            " -- 3.1705 bpw  accuracy: 0.98245664\n",
            " -- 4.0439 bpw  accuracy: 0.98831443\n",
            " -- 4.0471 bpw  accuracy: 0.98894177\n",
            " -- 4.0816 bpw  accuracy: 0.99001351\n",
            " -- 4.1381 bpw  accuracy: 0.99053100\n",
            " -- 4.1705 bpw  accuracy: 0.99116728\n",
            " -- 4.1902 bpw  accuracy: 0.99164836\n",
            " -- 4.2737 bpw  accuracy: 0.99226078\n",
            " -- 4.3295 bpw  accuracy: 0.99289807\n",
            " -- 5.2564 bpw  accuracy: 0.99573026\n",
            " -- 5.3295 bpw  accuracy: 0.99642581\n",
            " -- 6.0439 bpw  accuracy: 0.99697688\n",
            " -- 6.3381 bpw  accuracy: 0.99798415\n",
            " -- 8.0439 bpw  accuracy: 0.99919278\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.10 (Attention)       |\n",
            "| Duration: 11.93 seconds                     |\n",
            "| Completed step: 21/67                       |\n",
            "| Avg time / step (rolling): 22.88 seconds    |\n",
            "| Estimated remaining time: 17min 32sec       |\n",
            "| Last checkpoint layer: model.layers.7 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.10 (MLP)\n",
            " -- model.layers.10.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.10.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.10.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.10.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.10.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.10.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.10.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.10.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.95095831\n",
            " -- 2.3230 bpw  accuracy: 0.95246449\n",
            " -- 2.5958 bpw  accuracy: 0.96071340\n",
            " -- 2.9120 bpw  accuracy: 0.96325856\n",
            " -- 3.2833 bpw  accuracy: 0.97511408\n",
            " -- 3.3655 bpw  accuracy: 0.97720610\n",
            " -- 3.6186 bpw  accuracy: 0.98069898\n",
            " -- 4.1368 bpw  accuracy: 0.98684563\n",
            " -- 4.1977 bpw  accuracy: 0.98811348\n",
            " -- 4.2662 bpw  accuracy: 0.98729146\n",
            " -- 4.3484 bpw  accuracy: 0.98892895\n",
            " -- 5.2491 bpw  accuracy: 0.99352282\n",
            " -- 5.3313 bpw  accuracy: 0.99447654\n",
            " -- 6.0713 bpw  accuracy: 0.99643119\n",
            " -- 6.3032 bpw  accuracy: 0.99670919\n",
            " -- 6.8687 bpw  accuracy: 0.99744479\n",
            " -- 8.0354 bpw  accuracy: 0.99903249\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.10 (MLP)             |\n",
            "| Duration: 33.75 seconds                     |\n",
            "| Completed step: 22/67                       |\n",
            "| Avg time / step (rolling): 22.86 seconds    |\n",
            "| Estimated remaining time: 17min 8sec        |\n",
            "| Last checkpoint layer: model.layers.7 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.11 (Attention)\n",
            " -- model.layers.11.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.11.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.11.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.11.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.11.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.96092784\n",
            " -- 2.1987 bpw  accuracy: 0.96271768\n",
            " -- 2.2831 bpw  accuracy: 0.96625716\n",
            " -- 2.6768 bpw  accuracy: 0.97486000\n",
            " -- 3.1689 bpw  accuracy: 0.97986424\n",
            " -- 3.1705 bpw  accuracy: 0.98046008\n",
            " -- 4.0439 bpw  accuracy: 0.98610930\n",
            " -- 4.0471 bpw  accuracy: 0.98694848\n",
            " -- 4.0816 bpw  accuracy: 0.98803654\n",
            " -- 4.1381 bpw  accuracy: 0.98843495\n",
            " -- 4.1705 bpw  accuracy: 0.98997296\n",
            " -- 4.1902 bpw  accuracy: 0.99068295\n",
            " -- 4.2737 bpw  accuracy: 0.99122523\n",
            " -- 4.3295 bpw  accuracy: 0.99197524\n",
            " -- 5.2564 bpw  accuracy: 0.99494291\n",
            " -- 5.3295 bpw  accuracy: 0.99597608\n",
            " -- 6.0439 bpw  accuracy: 0.99612609\n",
            " -- 6.3381 bpw  accuracy: 0.99765912\n",
            " -- 8.0439 bpw  accuracy: 0.99900815\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.11 (Attention)       |\n",
            "| Duration: 11.98 seconds                     |\n",
            "| Completed step: 23/67                       |\n",
            "| Avg time / step (rolling): 22.87 seconds    |\n",
            "| Estimated remaining time: 16min 46sec       |\n",
            "| Last checkpoint layer: model.layers.7 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Layer: model.layers.11 (MLP)\n",
            " -- model.layers.11.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.11.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.11.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.11.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.11.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.11.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.11.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.11.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.94947620\n",
            " -- 2.3230 bpw  accuracy: 0.95105945\n",
            " -- 2.5958 bpw  accuracy: 0.95952494\n",
            " -- 2.9120 bpw  accuracy: 0.96215029\n",
            " -- 3.2833 bpw  accuracy: 0.97438584\n",
            " -- 3.3655 bpw  accuracy: 0.97652838\n",
            " -- 3.6186 bpw  accuracy: 0.98013617\n",
            " -- 4.1368 bpw  accuracy: 0.98642455\n",
            " -- 4.1977 bpw  accuracy: 0.98772960\n",
            " -- 4.2662 bpw  accuracy: 0.98692776\n",
            " -- 4.3484 bpw  accuracy: 0.98860136\n",
            " -- 5.2491 bpw  accuracy: 0.99333367\n",
            " -- 5.3313 bpw  accuracy: 0.99430986\n",
            " -- 6.0713 bpw  accuracy: 0.99630646\n",
            " -- 6.3032 bpw  accuracy: 0.99661869\n",
            " -- 6.8687 bpw  accuracy: 0.99737967\n",
            " -- 8.0354 bpw  accuracy: 0.99898830\n",
            "-----------------------------------------------\n",
            "| Measured: model.layers.11 (MLP)             |\n",
            "| Duration: 33.77 seconds                     |\n",
            "| Completed step: 24/67                       |\n",
            "| Avg time / step (rolling): 22.87 seconds    |\n",
            "| Estimated remaining time: 16min 23sec       |\n",
            "| Last checkpoint layer: model.layers.7 (MLP) |\n",
            "-----------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.12 (Attention)\n",
            " -- model.layers.12.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.12.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.12.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.12.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.12.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.95497149\n",
            " -- 2.1987 bpw  accuracy: 0.95729429\n",
            " -- 2.2831 bpw  accuracy: 0.96096700\n",
            " -- 2.6768 bpw  accuracy: 0.96914752\n",
            " -- 3.1689 bpw  accuracy: 0.97675191\n",
            " -- 3.1705 bpw  accuracy: 0.97728684\n",
            " -- 4.0439 bpw  accuracy: 0.98387393\n",
            " -- 4.0471 bpw  accuracy: 0.98448803\n",
            " -- 4.0816 bpw  accuracy: 0.98542907\n",
            " -- 4.1381 bpw  accuracy: 0.98632431\n",
            " -- 4.1705 bpw  accuracy: 0.98836104\n",
            " -- 4.1902 bpw  accuracy: 0.98919922\n",
            " -- 4.2737 bpw  accuracy: 0.98967935\n",
            " -- 4.3295 bpw  accuracy: 0.99051492\n",
            " -- 5.2564 bpw  accuracy: 0.99421612\n",
            " -- 5.3295 bpw  accuracy: 0.99520169\n",
            " -- 6.0439 bpw  accuracy: 0.99560382\n",
            " -- 6.3381 bpw  accuracy: 0.99738634\n",
            " -- 8.0439 bpw  accuracy: 0.99872250\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.12 (Attention)        |\n",
            "| Duration: 11.97 seconds                      |\n",
            "| Completed step: 25/67                        |\n",
            "| Avg time / step (rolling): 22.87 seconds     |\n",
            "| Estimated remaining time: 16min 0sec         |\n",
            "| Last checkpoint layer: model.layers.11 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.12 (MLP)\n",
            " -- model.layers.12.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.12.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.12.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.12.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.12.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.12.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.12.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.12.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.94482404\n",
            " -- 2.3230 bpw  accuracy: 0.94664729\n",
            " -- 2.5958 bpw  accuracy: 0.95549302\n",
            " -- 2.9120 bpw  accuracy: 0.95822299\n",
            " -- 3.2833 bpw  accuracy: 0.97204762\n",
            " -- 3.3655 bpw  accuracy: 0.97442339\n",
            " -- 3.6186 bpw  accuracy: 0.97817537\n",
            " -- 4.1368 bpw  accuracy: 0.98523562\n",
            " -- 4.1977 bpw  accuracy: 0.98662546\n",
            " -- 4.2662 bpw  accuracy: 0.98571101\n",
            " -- 4.3484 bpw  accuracy: 0.98756677\n",
            " -- 5.2491 bpw  accuracy: 0.99269979\n",
            " -- 5.3313 bpw  accuracy: 0.99377695\n",
            " -- 6.0713 bpw  accuracy: 0.99594282\n",
            " -- 6.3032 bpw  accuracy: 0.99628351\n",
            " -- 6.8687 bpw  accuracy: 0.99705449\n",
            " -- 8.0354 bpw  accuracy: 0.99887945\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.12 (MLP)              |\n",
            "| Duration: 33.86 seconds                      |\n",
            "| Completed step: 26/67                        |\n",
            "| Avg time / step (rolling): 22.88 seconds     |\n",
            "| Estimated remaining time: 15min 37sec        |\n",
            "| Last checkpoint layer: model.layers.11 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.13 (Attention)\n",
            " -- model.layers.13.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.13.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.13.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.13.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.13.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.95677763\n",
            " -- 2.1987 bpw  accuracy: 0.95863658\n",
            " -- 2.2831 bpw  accuracy: 0.96279997\n",
            " -- 2.6768 bpw  accuracy: 0.97267952\n",
            " -- 3.1689 bpw  accuracy: 0.97804663\n",
            " -- 3.1705 bpw  accuracy: 0.97853351\n",
            " -- 4.0439 bpw  accuracy: 0.98631115\n",
            " -- 4.0471 bpw  accuracy: 0.98702057\n",
            " -- 4.0816 bpw  accuracy: 0.98794433\n",
            " -- 4.1381 bpw  accuracy: 0.98848109\n",
            " -- 4.1705 bpw  accuracy: 0.98916386\n",
            " -- 4.1902 bpw  accuracy: 0.98979494\n",
            " -- 4.2737 bpw  accuracy: 0.99054718\n",
            " -- 4.3295 bpw  accuracy: 0.99128328\n",
            " -- 5.2564 bpw  accuracy: 0.99471482\n",
            " -- 5.3295 bpw  accuracy: 0.99564732\n",
            " -- 6.0439 bpw  accuracy: 0.99632491\n",
            " -- 6.3381 bpw  accuracy: 0.99759466\n",
            " -- 8.0439 bpw  accuracy: 0.99902323\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.13 (Attention)        |\n",
            "| Duration: 11.86 seconds                      |\n",
            "| Completed step: 27/67                        |\n",
            "| Avg time / step (rolling): 22.88 seconds     |\n",
            "| Estimated remaining time: 15min 15sec        |\n",
            "| Last checkpoint layer: model.layers.11 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.13 (MLP)\n",
            " -- model.layers.13.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.13.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.13.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.13.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.13.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.13.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.13.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.13.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.94136187\n",
            " -- 2.3230 bpw  accuracy: 0.94320825\n",
            " -- 2.5958 bpw  accuracy: 0.95275347\n",
            " -- 2.9120 bpw  accuracy: 0.95579984\n",
            " -- 3.2833 bpw  accuracy: 0.97011028\n",
            " -- 3.3655 bpw  accuracy: 0.97265031\n",
            " -- 3.6186 bpw  accuracy: 0.97675317\n",
            " -- 4.1368 bpw  accuracy: 0.98411054\n",
            " -- 4.1977 bpw  accuracy: 0.98563921\n",
            " -- 4.2662 bpw  accuracy: 0.98473041\n",
            " -- 4.3484 bpw  accuracy: 0.98670680\n",
            " -- 5.2491 bpw  accuracy: 0.99220833\n",
            " -- 5.3313 bpw  accuracy: 0.99336434\n",
            " -- 6.0713 bpw  accuracy: 0.99567370\n",
            " -- 6.3032 bpw  accuracy: 0.99603635\n",
            " -- 6.8687 bpw  accuracy: 0.99690129\n",
            " -- 8.0354 bpw  accuracy: 0.99881352\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.13 (MLP)              |\n",
            "| Duration: 33.78 seconds                      |\n",
            "| Completed step: 28/67                        |\n",
            "| Avg time / step (rolling): 22.88 seconds     |\n",
            "| Estimated remaining time: 14min 52sec        |\n",
            "| Last checkpoint layer: model.layers.11 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.14 (Attention)\n",
            " -- model.layers.14.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.14.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.14.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.14.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.14.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.95142432\n",
            " -- 2.1987 bpw  accuracy: 0.95376446\n",
            " -- 2.2831 bpw  accuracy: 0.95841915\n",
            " -- 2.6768 bpw  accuracy: 0.96904453\n",
            " -- 3.1689 bpw  accuracy: 0.97473409\n",
            " -- 3.1705 bpw  accuracy: 0.97551306\n",
            " -- 4.0439 bpw  accuracy: 0.98363912\n",
            " -- 4.0471 bpw  accuracy: 0.98478491\n",
            " -- 4.0816 bpw  accuracy: 0.98584172\n",
            " -- 4.1381 bpw  accuracy: 0.98622305\n",
            " -- 4.1705 bpw  accuracy: 0.98736817\n",
            " -- 4.1902 bpw  accuracy: 0.98815438\n",
            " -- 4.2737 bpw  accuracy: 0.98897412\n",
            " -- 4.3295 bpw  accuracy: 0.98997909\n",
            " -- 5.2564 bpw  accuracy: 0.99362657\n",
            " -- 5.3295 bpw  accuracy: 0.99493921\n",
            " -- 6.0439 bpw  accuracy: 0.99524748\n",
            " -- 6.3381 bpw  accuracy: 0.99722645\n",
            " -- 8.0439 bpw  accuracy: 0.99871751\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.14 (Attention)        |\n",
            "| Duration: 11.91 seconds                      |\n",
            "| Completed step: 29/67                        |\n",
            "| Avg time / step (rolling): 22.87 seconds     |\n",
            "| Estimated remaining time: 14min 29sec        |\n",
            "| Last checkpoint layer: model.layers.11 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.14 (MLP)\n",
            " -- model.layers.14.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.14.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.14.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.14.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.14.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.14.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.14.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.14.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.93690496\n",
            " -- 2.3230 bpw  accuracy: 0.93886134\n",
            " -- 2.5958 bpw  accuracy: 0.94893391\n",
            " -- 2.9120 bpw  accuracy: 0.95213144\n",
            " -- 3.2833 bpw  accuracy: 0.96797503\n",
            " -- 3.3655 bpw  accuracy: 0.97084171\n",
            " -- 3.6186 bpw  accuracy: 0.97517228\n",
            " -- 4.1368 bpw  accuracy: 0.98287856\n",
            " -- 4.1977 bpw  accuracy: 0.98458308\n",
            " -- 4.2662 bpw  accuracy: 0.98370034\n",
            " -- 4.3484 bpw  accuracy: 0.98578446\n",
            " -- 5.2491 bpw  accuracy: 0.99165915\n",
            " -- 5.3313 bpw  accuracy: 0.99286213\n",
            " -- 6.0713 bpw  accuracy: 0.99531910\n",
            " -- 6.3032 bpw  accuracy: 0.99573994\n",
            " -- 6.8687 bpw  accuracy: 0.99665615\n",
            " -- 8.0354 bpw  accuracy: 0.99864366\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.14 (MLP)              |\n",
            "| Duration: 33.73 seconds                      |\n",
            "| Completed step: 30/67                        |\n",
            "| Avg time / step (rolling): 22.85 seconds     |\n",
            "| Estimated remaining time: 14min 5sec         |\n",
            "| Last checkpoint layer: model.layers.11 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.15 (Attention)\n",
            " -- model.layers.15.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.15.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.15.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.15.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.15.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94746189\n",
            " -- 2.1987 bpw  accuracy: 0.94934057\n",
            " -- 2.2831 bpw  accuracy: 0.95420918\n",
            " -- 2.6768 bpw  accuracy: 0.96364619\n",
            " -- 3.1689 bpw  accuracy: 0.97205734\n",
            " -- 3.1705 bpw  accuracy: 0.97292173\n",
            " -- 4.0439 bpw  accuracy: 0.98082847\n",
            " -- 4.0471 bpw  accuracy: 0.98186975\n",
            " -- 4.0816 bpw  accuracy: 0.98324078\n",
            " -- 4.1381 bpw  accuracy: 0.98430775\n",
            " -- 4.1705 bpw  accuracy: 0.98578498\n",
            " -- 4.1902 bpw  accuracy: 0.98682765\n",
            " -- 4.2737 bpw  accuracy: 0.98746150\n",
            " -- 4.3295 bpw  accuracy: 0.98875030\n",
            " -- 5.2564 bpw  accuracy: 0.99319441\n",
            " -- 5.3295 bpw  accuracy: 0.99443277\n",
            " -- 6.0439 bpw  accuracy: 0.99478138\n",
            " -- 6.3381 bpw  accuracy: 0.99698963\n",
            " -- 8.0439 bpw  accuracy: 0.99858972\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.15 (Attention)        |\n",
            "| Duration: 11.88 seconds                      |\n",
            "| Completed step: 31/67                        |\n",
            "| Avg time / step (rolling): 22.85 seconds     |\n",
            "| Estimated remaining time: 13min 42sec        |\n",
            "| Last checkpoint layer: model.layers.11 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.15 (MLP)\n",
            " -- model.layers.15.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.15.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.15.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.15.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.15.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.15.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.15.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.15.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.93268816\n",
            " -- 2.3230 bpw  accuracy: 0.93478254\n",
            " -- 2.5958 bpw  accuracy: 0.94569535\n",
            " -- 2.9120 bpw  accuracy: 0.94912148\n",
            " -- 3.2833 bpw  accuracy: 0.96591628\n",
            " -- 3.3655 bpw  accuracy: 0.96870893\n",
            " -- 3.6186 bpw  accuracy: 0.97336368\n",
            " -- 4.1368 bpw  accuracy: 0.98195666\n",
            " -- 4.1977 bpw  accuracy: 0.98360634\n",
            " -- 4.2662 bpw  accuracy: 0.98261731\n",
            " -- 4.3484 bpw  accuracy: 0.98480396\n",
            " -- 5.2491 bpw  accuracy: 0.99113158\n",
            " -- 5.3313 bpw  accuracy: 0.99241681\n",
            " -- 6.0713 bpw  accuracy: 0.99508634\n",
            " -- 6.3032 bpw  accuracy: 0.99549702\n",
            " -- 6.8687 bpw  accuracy: 0.99646459\n",
            " -- 8.0354 bpw  accuracy: 0.99865663\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.15 (MLP)              |\n",
            "| Duration: 33.71 seconds                      |\n",
            "| Completed step: 32/67                        |\n",
            "| Avg time / step (rolling): 22.84 seconds     |\n",
            "| Estimated remaining time: 13min 19sec        |\n",
            "| Last checkpoint layer: model.layers.11 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.16 (Attention)\n",
            " -- model.layers.16.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.16.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.16.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.16.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.16.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.95050338\n",
            " -- 2.1987 bpw  accuracy: 0.95214220\n",
            " -- 2.2831 bpw  accuracy: 0.95751995\n",
            " -- 2.6768 bpw  accuracy: 0.96765671\n",
            " -- 3.1689 bpw  accuracy: 0.97444186\n",
            " -- 3.1705 bpw  accuracy: 0.97512672\n",
            " -- 4.0439 bpw  accuracy: 0.98399732\n",
            " -- 4.0471 bpw  accuracy: 0.98493529\n",
            " -- 4.0816 bpw  accuracy: 0.98596075\n",
            " -- 4.1381 bpw  accuracy: 0.98670501\n",
            " -- 4.1705 bpw  accuracy: 0.98724487\n",
            " -- 4.1902 bpw  accuracy: 0.98799390\n",
            " -- 4.2737 bpw  accuracy: 0.98904557\n",
            " -- 4.3295 bpw  accuracy: 0.98993798\n",
            " -- 5.2564 bpw  accuracy: 0.99388331\n",
            " -- 5.3295 bpw  accuracy: 0.99499059\n",
            " -- 6.0439 bpw  accuracy: 0.99562792\n",
            " -- 6.3381 bpw  accuracy: 0.99727731\n",
            " -- 8.0439 bpw  accuracy: 0.99882059\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.16 (Attention)        |\n",
            "| Duration: 11.83 seconds                      |\n",
            "| Completed step: 33/67                        |\n",
            "| Avg time / step (rolling): 22.83 seconds     |\n",
            "| Estimated remaining time: 12min 56sec        |\n",
            "| Last checkpoint layer: model.layers.15 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.16 (MLP)\n",
            " -- model.layers.16.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.16.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.16.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.16.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.16.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.16.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.16.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.16.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.92663227\n",
            " -- 2.3230 bpw  accuracy: 0.92892432\n",
            " -- 2.5958 bpw  accuracy: 0.94076666\n",
            " -- 2.9120 bpw  accuracy: 0.94462212\n",
            " -- 3.2833 bpw  accuracy: 0.96285911\n",
            " -- 3.3655 bpw  accuracy: 0.96592391\n",
            " -- 3.6186 bpw  accuracy: 0.97099635\n",
            " -- 4.1368 bpw  accuracy: 0.98016092\n",
            " -- 4.1977 bpw  accuracy: 0.98199537\n",
            " -- 4.2662 bpw  accuracy: 0.98105669\n",
            " -- 4.3484 bpw  accuracy: 0.98345359\n",
            " -- 5.2491 bpw  accuracy: 0.99034267\n",
            " -- 5.3313 bpw  accuracy: 0.99173820\n",
            " -- 6.0713 bpw  accuracy: 0.99458522\n",
            " -- 6.3032 bpw  accuracy: 0.99510760\n",
            " -- 6.8687 bpw  accuracy: 0.99616517\n",
            " -- 8.0354 bpw  accuracy: 0.99849359\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.16 (MLP)              |\n",
            "| Duration: 33.73 seconds                      |\n",
            "| Completed step: 34/67                        |\n",
            "| Avg time / step (rolling): 22.83 seconds     |\n",
            "| Estimated remaining time: 12min 33sec        |\n",
            "| Last checkpoint layer: model.layers.15 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.17 (Attention)\n",
            " -- model.layers.17.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.17.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.17.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.17.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.17.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94687639\n",
            " -- 2.1987 bpw  accuracy: 0.94920247\n",
            " -- 2.2831 bpw  accuracy: 0.95373578\n",
            " -- 2.6768 bpw  accuracy: 0.96568864\n",
            " -- 3.1689 bpw  accuracy: 0.97209875\n",
            " -- 3.1705 bpw  accuracy: 0.97281114\n",
            " -- 4.0439 bpw  accuracy: 0.98125367\n",
            " -- 4.0471 bpw  accuracy: 0.98222374\n",
            " -- 4.0816 bpw  accuracy: 0.98409547\n",
            " -- 4.1381 bpw  accuracy: 0.98476382\n",
            " -- 4.1705 bpw  accuracy: 0.98618766\n",
            " -- 4.1902 bpw  accuracy: 0.98705464\n",
            " -- 4.2737 bpw  accuracy: 0.98775989\n",
            " -- 4.3295 bpw  accuracy: 0.98874353\n",
            " -- 5.2564 bpw  accuracy: 0.99290636\n",
            " -- 5.3295 bpw  accuracy: 0.99435643\n",
            " -- 6.0439 bpw  accuracy: 0.99466827\n",
            " -- 6.3381 bpw  accuracy: 0.99701204\n",
            " -- 8.0439 bpw  accuracy: 0.99865724\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.17 (Attention)        |\n",
            "| Duration: 11.95 seconds                      |\n",
            "| Completed step: 35/67                        |\n",
            "| Avg time / step (rolling): 22.82 seconds     |\n",
            "| Estimated remaining time: 12min 10sec        |\n",
            "| Last checkpoint layer: model.layers.15 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.17 (MLP)\n",
            " -- model.layers.17.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.17.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.17.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.17.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.17.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.17.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.17.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.17.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.92048443\n",
            " -- 2.3230 bpw  accuracy: 0.92303085\n",
            " -- 2.5958 bpw  accuracy: 0.93578399\n",
            " -- 2.9120 bpw  accuracy: 0.94030747\n",
            " -- 3.2833 bpw  accuracy: 0.95961078\n",
            " -- 3.3655 bpw  accuracy: 0.96295526\n",
            " -- 3.6186 bpw  accuracy: 0.96854504\n",
            " -- 4.1368 bpw  accuracy: 0.97818065\n",
            " -- 4.1977 bpw  accuracy: 0.98028153\n",
            " -- 4.2662 bpw  accuracy: 0.97935935\n",
            " -- 4.3484 bpw  accuracy: 0.98198899\n",
            " -- 5.2491 bpw  accuracy: 0.98946012\n",
            " -- 5.3313 bpw  accuracy: 0.99100005\n",
            " -- 6.0713 bpw  accuracy: 0.99404954\n",
            " -- 6.3032 bpw  accuracy: 0.99466461\n",
            " -- 6.8687 bpw  accuracy: 0.99586620\n",
            " -- 8.0354 bpw  accuracy: 0.99833643\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.17 (MLP)              |\n",
            "| Duration: 33.94 seconds                      |\n",
            "| Completed step: 36/67                        |\n",
            "| Avg time / step (rolling): 22.83 seconds     |\n",
            "| Estimated remaining time: 11min 47sec        |\n",
            "| Last checkpoint layer: model.layers.15 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.18 (Attention)\n",
            " -- model.layers.18.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.18.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.18.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.18.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.18.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94710512\n",
            " -- 2.1987 bpw  accuracy: 0.94934163\n",
            " -- 2.2831 bpw  accuracy: 0.95427913\n",
            " -- 2.6768 bpw  accuracy: 0.96435621\n",
            " -- 3.1689 bpw  accuracy: 0.97270342\n",
            " -- 3.1705 bpw  accuracy: 0.97349691\n",
            " -- 4.0439 bpw  accuracy: 0.98137824\n",
            " -- 4.0471 bpw  accuracy: 0.98248535\n",
            " -- 4.0816 bpw  accuracy: 0.98321875\n",
            " -- 4.1381 bpw  accuracy: 0.98440606\n",
            " -- 4.1705 bpw  accuracy: 0.98635639\n",
            " -- 4.1902 bpw  accuracy: 0.98716228\n",
            " -- 4.2737 bpw  accuracy: 0.98782826\n",
            " -- 4.3295 bpw  accuracy: 0.98894448\n",
            " -- 5.2564 bpw  accuracy: 0.99218715\n",
            " -- 5.3295 bpw  accuracy: 0.99431925\n",
            " -- 6.0439 bpw  accuracy: 0.99354872\n",
            " -- 6.3381 bpw  accuracy: 0.99704737\n",
            " -- 8.0439 bpw  accuracy: 0.99823556\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.18 (Attention)        |\n",
            "| Duration: 11.93 seconds                      |\n",
            "| Completed step: 37/67                        |\n",
            "| Avg time / step (rolling): 22.84 seconds     |\n",
            "| Estimated remaining time: 11min 25sec        |\n",
            "| Last checkpoint layer: model.layers.15 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.18 (MLP)\n",
            " -- model.layers.18.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.18.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.18.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.18.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.18.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.18.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.18.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.18.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91510076\n",
            " -- 2.3230 bpw  accuracy: 0.91783840\n",
            " -- 2.5958 bpw  accuracy: 0.93154725\n",
            " -- 2.9120 bpw  accuracy: 0.93654935\n",
            " -- 3.2833 bpw  accuracy: 0.95695041\n",
            " -- 3.3655 bpw  accuracy: 0.96051874\n",
            " -- 3.6186 bpw  accuracy: 0.96652429\n",
            " -- 4.1368 bpw  accuracy: 0.97655404\n",
            " -- 4.1977 bpw  accuracy: 0.97883857\n",
            " -- 4.2662 bpw  accuracy: 0.97800263\n",
            " -- 4.3484 bpw  accuracy: 0.98082096\n",
            " -- 5.2491 bpw  accuracy: 0.98876678\n",
            " -- 5.3313 bpw  accuracy: 0.99041427\n",
            " -- 6.0713 bpw  accuracy: 0.99360107\n",
            " -- 6.3032 bpw  accuracy: 0.99430635\n",
            " -- 6.8687 bpw  accuracy: 0.99560812\n",
            " -- 8.0354 bpw  accuracy: 0.99816977\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.18 (MLP)              |\n",
            "| Duration: 33.68 seconds                      |\n",
            "| Completed step: 38/67                        |\n",
            "| Avg time / step (rolling): 22.83 seconds     |\n",
            "| Estimated remaining time: 11min 2sec         |\n",
            "| Last checkpoint layer: model.layers.15 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.19 (Attention)\n",
            " -- model.layers.19.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.19.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.19.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.19.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.19.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94883888\n",
            " -- 2.1987 bpw  accuracy: 0.95066694\n",
            " -- 2.2831 bpw  accuracy: 0.95528290\n",
            " -- 2.6768 bpw  accuracy: 0.96576602\n",
            " -- 3.1689 bpw  accuracy: 0.97351192\n",
            " -- 3.1705 bpw  accuracy: 0.97433013\n",
            " -- 4.0439 bpw  accuracy: 0.98241880\n",
            " -- 4.0471 bpw  accuracy: 0.98351122\n",
            " -- 4.0816 bpw  accuracy: 0.98480575\n",
            " -- 4.1381 bpw  accuracy: 0.98549543\n",
            " -- 4.1705 bpw  accuracy: 0.98690823\n",
            " -- 4.1902 bpw  accuracy: 0.98767505\n",
            " -- 4.2737 bpw  accuracy: 0.98855560\n",
            " -- 4.3295 bpw  accuracy: 0.98926044\n",
            " -- 5.2564 bpw  accuracy: 0.99350095\n",
            " -- 5.3295 bpw  accuracy: 0.99476808\n",
            " -- 6.0439 bpw  accuracy: 0.99522363\n",
            " -- 6.3381 bpw  accuracy: 0.99710168\n",
            " -- 8.0439 bpw  accuracy: 0.99872043\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.19 (Attention)        |\n",
            "| Duration: 12.09 seconds                      |\n",
            "| Completed step: 39/67                        |\n",
            "| Avg time / step (rolling): 22.85 seconds     |\n",
            "| Estimated remaining time: 10min 39sec        |\n",
            "| Last checkpoint layer: model.layers.15 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.19 (MLP)\n",
            " -- model.layers.19.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.19.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.19.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.19.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.19.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.19.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.19.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.19.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91353916\n",
            " -- 2.3230 bpw  accuracy: 0.91625298\n",
            " -- 2.5958 bpw  accuracy: 0.93013112\n",
            " -- 2.9120 bpw  accuracy: 0.93520242\n",
            " -- 3.2833 bpw  accuracy: 0.95600237\n",
            " -- 3.3655 bpw  accuracy: 0.95973639\n",
            " -- 3.6186 bpw  accuracy: 0.96590915\n",
            " -- 4.1368 bpw  accuracy: 0.97611589\n",
            " -- 4.1977 bpw  accuracy: 0.97844007\n",
            " -- 4.2662 bpw  accuracy: 0.97746584\n",
            " -- 4.3484 bpw  accuracy: 0.98043700\n",
            " -- 5.2491 bpw  accuracy: 0.98846898\n",
            " -- 5.3313 bpw  accuracy: 0.99021072\n",
            " -- 6.0713 bpw  accuracy: 0.99345775\n",
            " -- 6.3032 bpw  accuracy: 0.99415462\n",
            " -- 6.8687 bpw  accuracy: 0.99547240\n",
            " -- 8.0354 bpw  accuracy: 0.99813883\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.19 (MLP)              |\n",
            "| Duration: 33.84 seconds                      |\n",
            "| Completed step: 40/67                        |\n",
            "| Avg time / step (rolling): 22.86 seconds     |\n",
            "| Estimated remaining time: 10min 17sec        |\n",
            "| Last checkpoint layer: model.layers.15 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.20 (Attention)\n",
            " -- model.layers.20.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.20.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.20.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.20.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.20.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.95325135\n",
            " -- 2.1987 bpw  accuracy: 0.95517738\n",
            " -- 2.2831 bpw  accuracy: 0.95910592\n",
            " -- 2.6768 bpw  accuracy: 0.96907891\n",
            " -- 3.1689 bpw  accuracy: 0.97556290\n",
            " -- 3.1705 bpw  accuracy: 0.97649772\n",
            " -- 4.0439 bpw  accuracy: 0.98323175\n",
            " -- 4.0471 bpw  accuracy: 0.98457339\n",
            " -- 4.0816 bpw  accuracy: 0.98574651\n",
            " -- 4.1381 bpw  accuracy: 0.98617045\n",
            " -- 4.1705 bpw  accuracy: 0.98794535\n",
            " -- 4.1902 bpw  accuracy: 0.98876411\n",
            " -- 4.2737 bpw  accuracy: 0.98934186\n",
            " -- 4.3295 bpw  accuracy: 0.99017358\n",
            " -- 5.2564 bpw  accuracy: 0.99369634\n",
            " -- 5.3295 bpw  accuracy: 0.99502795\n",
            " -- 6.0439 bpw  accuracy: 0.99511157\n",
            " -- 6.3381 bpw  accuracy: 0.99723039\n",
            " -- 8.0439 bpw  accuracy: 0.99869138\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.20 (Attention)        |\n",
            "| Duration: 12.05 seconds                      |\n",
            "| Completed step: 41/67                        |\n",
            "| Avg time / step (rolling): 22.88 seconds     |\n",
            "| Estimated remaining time: 9min 54sec         |\n",
            "| Last checkpoint layer: model.layers.19 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.20 (MLP)\n",
            " -- model.layers.20.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.20.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.20.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.20.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.20.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.20.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.20.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.20.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91392138\n",
            " -- 2.3230 bpw  accuracy: 0.91645644\n",
            " -- 2.5958 bpw  accuracy: 0.93007983\n",
            " -- 2.9120 bpw  accuracy: 0.93473345\n",
            " -- 3.2833 bpw  accuracy: 0.95637757\n",
            " -- 3.3655 bpw  accuracy: 0.95989519\n",
            " -- 3.6186 bpw  accuracy: 0.96588621\n",
            " -- 4.1368 bpw  accuracy: 0.97673912\n",
            " -- 4.1977 bpw  accuracy: 0.97888979\n",
            " -- 4.2662 bpw  accuracy: 0.97776568\n",
            " -- 4.3484 bpw  accuracy: 0.98056662\n",
            " -- 5.2491 bpw  accuracy: 0.98866295\n",
            " -- 5.3313 bpw  accuracy: 0.99030392\n",
            " -- 6.0713 bpw  accuracy: 0.99368742\n",
            " -- 6.3032 bpw  accuracy: 0.99425300\n",
            " -- 6.8687 bpw  accuracy: 0.99550527\n",
            " -- 8.0354 bpw  accuracy: 0.99822635\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.20 (MLP)              |\n",
            "| Duration: 33.92 seconds                      |\n",
            "| Completed step: 42/67                        |\n",
            "| Avg time / step (rolling): 22.90 seconds     |\n",
            "| Estimated remaining time: 9min 32sec         |\n",
            "| Last checkpoint layer: model.layers.19 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.21 (Attention)\n",
            " -- model.layers.21.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.21.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.21.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.21.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.21.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.95784130\n",
            " -- 2.1987 bpw  accuracy: 0.95949705\n",
            " -- 2.2831 bpw  accuracy: 0.96356448\n",
            " -- 2.6768 bpw  accuracy: 0.97204646\n",
            " -- 3.1689 bpw  accuracy: 0.97850821\n",
            " -- 3.1705 bpw  accuracy: 0.97901424\n",
            " -- 4.0439 bpw  accuracy: 0.98517701\n",
            " -- 4.0471 bpw  accuracy: 0.98583565\n",
            " -- 4.0816 bpw  accuracy: 0.98708409\n",
            " -- 4.1381 bpw  accuracy: 0.98752869\n",
            " -- 4.1705 bpw  accuracy: 0.98882703\n",
            " -- 4.1902 bpw  accuracy: 0.98967209\n",
            " -- 4.2737 bpw  accuracy: 0.99008772\n",
            " -- 4.3295 bpw  accuracy: 0.99093437\n",
            " -- 5.2564 bpw  accuracy: 0.99423382\n",
            " -- 5.3295 bpw  accuracy: 0.99537784\n",
            " -- 6.0439 bpw  accuracy: 0.99536919\n",
            " -- 6.3381 bpw  accuracy: 0.99759657\n",
            " -- 8.0439 bpw  accuracy: 0.99880626\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.21 (Attention)        |\n",
            "| Duration: 11.92 seconds                      |\n",
            "| Completed step: 43/67                        |\n",
            "| Avg time / step (rolling): 22.91 seconds     |\n",
            "| Estimated remaining time: 9min 9sec          |\n",
            "| Last checkpoint layer: model.layers.19 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.21 (MLP)\n",
            " -- model.layers.21.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.21.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.21.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.21.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.21.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.21.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.21.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.21.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91480194\n",
            " -- 2.3230 bpw  accuracy: 0.91726028\n",
            " -- 2.5958 bpw  accuracy: 0.93045919\n",
            " -- 2.9120 bpw  accuracy: 0.93470979\n",
            " -- 3.2833 bpw  accuracy: 0.95691981\n",
            " -- 3.3655 bpw  accuracy: 0.96032175\n",
            " -- 3.6186 bpw  accuracy: 0.96605713\n",
            " -- 4.1368 bpw  accuracy: 0.97731551\n",
            " -- 4.1977 bpw  accuracy: 0.97933332\n",
            " -- 4.2662 bpw  accuracy: 0.97808115\n",
            " -- 4.3484 bpw  accuracy: 0.98078477\n",
            " -- 5.2491 bpw  accuracy: 0.98883980\n",
            " -- 5.3313 bpw  accuracy: 0.99042296\n",
            " -- 6.0713 bpw  accuracy: 0.99384326\n",
            " -- 6.3032 bpw  accuracy: 0.99432978\n",
            " -- 6.8687 bpw  accuracy: 0.99547315\n",
            " -- 8.0354 bpw  accuracy: 0.99828143\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.21 (MLP)              |\n",
            "| Duration: 34.03 seconds                      |\n",
            "| Completed step: 44/67                        |\n",
            "| Avg time / step (rolling): 22.94 seconds     |\n",
            "| Estimated remaining time: 8min 47sec         |\n",
            "| Last checkpoint layer: model.layers.19 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.22 (Attention)\n",
            " -- model.layers.22.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.22.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.22.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.22.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.22.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.96461019\n",
            " -- 2.1987 bpw  accuracy: 0.96636222\n",
            " -- 2.2831 bpw  accuracy: 0.97005697\n",
            " -- 2.6768 bpw  accuracy: 0.97613398\n",
            " -- 3.1689 bpw  accuracy: 0.98043602\n",
            " -- 3.1705 bpw  accuracy: 0.98131932\n",
            " -- 4.0439 bpw  accuracy: 0.98512823\n",
            " -- 4.0471 bpw  accuracy: 0.98625679\n",
            " -- 4.0816 bpw  accuracy: 0.98792747\n",
            " -- 4.1381 bpw  accuracy: 0.98848560\n",
            " -- 4.1705 bpw  accuracy: 0.99042241\n",
            " -- 4.1902 bpw  accuracy: 0.99103883\n",
            " -- 4.2737 bpw  accuracy: 0.99134844\n",
            " -- 4.3295 bpw  accuracy: 0.99216267\n",
            " -- 5.2564 bpw  accuracy: 0.99475117\n",
            " -- 5.3295 bpw  accuracy: 0.99602106\n",
            " -- 6.0439 bpw  accuracy: 0.99551607\n",
            " -- 6.3381 bpw  accuracy: 0.99799308\n",
            " -- 8.0439 bpw  accuracy: 0.99881047\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.22 (Attention)        |\n",
            "| Duration: 11.97 seconds                      |\n",
            "| Completed step: 45/67                        |\n",
            "| Avg time / step (rolling): 22.94 seconds     |\n",
            "| Estimated remaining time: 8min 24sec         |\n",
            "| Last checkpoint layer: model.layers.19 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.22 (MLP)\n",
            " -- model.layers.22.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.22.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.22.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.22.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.22.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.22.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.22.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.22.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91549705\n",
            " -- 2.3230 bpw  accuracy: 0.91785504\n",
            " -- 2.5958 bpw  accuracy: 0.93055295\n",
            " -- 2.9120 bpw  accuracy: 0.93454512\n",
            " -- 3.2833 bpw  accuracy: 0.95715535\n",
            " -- 3.3655 bpw  accuracy: 0.96051844\n",
            " -- 3.6186 bpw  accuracy: 0.96599915\n",
            " -- 4.1368 bpw  accuracy: 0.97749786\n",
            " -- 4.1977 bpw  accuracy: 0.97949812\n",
            " -- 4.2662 bpw  accuracy: 0.97817219\n",
            " -- 4.3484 bpw  accuracy: 0.98086884\n",
            " -- 5.2491 bpw  accuracy: 0.98890260\n",
            " -- 5.3313 bpw  accuracy: 0.99047213\n",
            " -- 6.0713 bpw  accuracy: 0.99392308\n",
            " -- 6.3032 bpw  accuracy: 0.99436556\n",
            " -- 6.8687 bpw  accuracy: 0.99545862\n",
            " -- 8.0354 bpw  accuracy: 0.99831266\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.22 (MLP)              |\n",
            "| Duration: 34.10 seconds                      |\n",
            "| Completed step: 46/67                        |\n",
            "| Avg time / step (rolling): 22.95 seconds     |\n",
            "| Estimated remaining time: 8min 2sec          |\n",
            "| Last checkpoint layer: model.layers.19 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.23 (Attention)\n",
            " -- model.layers.23.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.23.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.23.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.23.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.23.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.96144368\n",
            " -- 2.1987 bpw  accuracy: 0.96321644\n",
            " -- 2.2831 bpw  accuracy: 0.96714985\n",
            " -- 2.6768 bpw  accuracy: 0.97383911\n",
            " -- 3.1689 bpw  accuracy: 0.97893453\n",
            " -- 3.1705 bpw  accuracy: 0.98010537\n",
            " -- 4.0439 bpw  accuracy: 0.98434053\n",
            " -- 4.0471 bpw  accuracy: 0.98590600\n",
            " -- 4.0816 bpw  accuracy: 0.98705028\n",
            " -- 4.1381 bpw  accuracy: 0.98767805\n",
            " -- 4.1705 bpw  accuracy: 0.98999631\n",
            " -- 4.1902 bpw  accuracy: 0.99065767\n",
            " -- 4.2737 bpw  accuracy: 0.99120639\n",
            " -- 4.3295 bpw  accuracy: 0.99190866\n",
            " -- 5.2564 bpw  accuracy: 0.99473721\n",
            " -- 5.3295 bpw  accuracy: 0.99599179\n",
            " -- 6.0439 bpw  accuracy: 0.99565781\n",
            " -- 6.3381 bpw  accuracy: 0.99789982\n",
            " -- 8.0439 bpw  accuracy: 0.99887381\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.23 (Attention)        |\n",
            "| Duration: 12.06 seconds                      |\n",
            "| Completed step: 47/67                        |\n",
            "| Avg time / step (rolling): 22.97 seconds     |\n",
            "| Estimated remaining time: 7min 39sec         |\n",
            "| Last checkpoint layer: model.layers.19 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.23 (MLP)\n",
            " -- model.layers.23.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.23.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.23.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.23.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.23.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.23.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.23.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.23.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91510165\n",
            " -- 2.3230 bpw  accuracy: 0.91747099\n",
            " -- 2.5958 bpw  accuracy: 0.92978290\n",
            " -- 2.9120 bpw  accuracy: 0.93366610\n",
            " -- 3.2833 bpw  accuracy: 0.95690619\n",
            " -- 3.3655 bpw  accuracy: 0.96029718\n",
            " -- 3.6186 bpw  accuracy: 0.96564207\n",
            " -- 4.1368 bpw  accuracy: 0.97737368\n",
            " -- 4.1977 bpw  accuracy: 0.97937206\n",
            " -- 4.2662 bpw  accuracy: 0.97805914\n",
            " -- 4.3484 bpw  accuracy: 0.98075591\n",
            " -- 5.2491 bpw  accuracy: 0.98883342\n",
            " -- 5.3313 bpw  accuracy: 0.99041751\n",
            " -- 6.0713 bpw  accuracy: 0.99387568\n",
            " -- 6.3032 bpw  accuracy: 0.99432397\n",
            " -- 6.8687 bpw  accuracy: 0.99537077\n",
            " -- 8.0354 bpw  accuracy: 0.99829127\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.23 (MLP)              |\n",
            "| Duration: 33.97 seconds                      |\n",
            "| Completed step: 48/67                        |\n",
            "| Avg time / step (rolling): 23.00 seconds     |\n",
            "| Estimated remaining time: 7min 16sec         |\n",
            "| Last checkpoint layer: model.layers.19 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.24 (Attention)\n",
            " -- model.layers.24.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.24.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.24.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.24.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.24.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.96257212\n",
            " -- 2.1987 bpw  accuracy: 0.96409544\n",
            " -- 2.2831 bpw  accuracy: 0.96815047\n",
            " -- 2.6768 bpw  accuracy: 0.97486615\n",
            " -- 3.1689 bpw  accuracy: 0.97995368\n",
            " -- 3.1705 bpw  accuracy: 0.98089216\n",
            " -- 4.0439 bpw  accuracy: 0.98570531\n",
            " -- 4.0471 bpw  accuracy: 0.98689639\n",
            " -- 4.0816 bpw  accuracy: 0.98764440\n",
            " -- 4.1381 bpw  accuracy: 0.98846461\n",
            " -- 4.1705 bpw  accuracy: 0.99020582\n",
            " -- 4.1902 bpw  accuracy: 0.99094274\n",
            " -- 4.2737 bpw  accuracy: 0.99156304\n",
            " -- 4.3295 bpw  accuracy: 0.99228689\n",
            " -- 5.2564 bpw  accuracy: 0.99513498\n",
            " -- 5.3295 bpw  accuracy: 0.99612904\n",
            " -- 6.0439 bpw  accuracy: 0.99606761\n",
            " -- 6.3381 bpw  accuracy: 0.99787116\n",
            " -- 8.0439 bpw  accuracy: 0.99891893\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.24 (Attention)        |\n",
            "| Duration: 12.01 seconds                      |\n",
            "| Completed step: 49/67                        |\n",
            "| Avg time / step (rolling): 22.99 seconds     |\n",
            "| Estimated remaining time: 6min 53sec         |\n",
            "| Last checkpoint layer: model.layers.23 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.24 (MLP)\n",
            " -- model.layers.24.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.24.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.24.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.24.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.24.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.24.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.24.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.24.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91491852\n",
            " -- 2.3230 bpw  accuracy: 0.91733397\n",
            " -- 2.5958 bpw  accuracy: 0.92939297\n",
            " -- 2.9120 bpw  accuracy: 0.93317702\n",
            " -- 3.2833 bpw  accuracy: 0.95686523\n",
            " -- 3.3655 bpw  accuracy: 0.96027245\n",
            " -- 3.6186 bpw  accuracy: 0.96548226\n",
            " -- 4.1368 bpw  accuracy: 0.97737771\n",
            " -- 4.1977 bpw  accuracy: 0.97933167\n",
            " -- 4.2662 bpw  accuracy: 0.97802880\n",
            " -- 4.3484 bpw  accuracy: 0.98074737\n",
            " -- 5.2491 bpw  accuracy: 0.98882752\n",
            " -- 5.3313 bpw  accuracy: 0.99041133\n",
            " -- 6.0713 bpw  accuracy: 0.99385661\n",
            " -- 6.3032 bpw  accuracy: 0.99432461\n",
            " -- 6.8687 bpw  accuracy: 0.99533995\n",
            " -- 8.0354 bpw  accuracy: 0.99828698\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.24 (MLP)              |\n",
            "| Duration: 33.87 seconds                      |\n",
            "| Completed step: 50/67                        |\n",
            "| Avg time / step (rolling): 22.99 seconds     |\n",
            "| Estimated remaining time: 6min 30sec         |\n",
            "| Last checkpoint layer: model.layers.23 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.25 (Attention)\n",
            " -- model.layers.25.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.25.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.25.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.25.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.25.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.96120341\n",
            " -- 2.1987 bpw  accuracy: 0.96268385\n",
            " -- 2.2831 bpw  accuracy: 0.96738063\n",
            " -- 2.6768 bpw  accuracy: 0.97368564\n",
            " -- 3.1689 bpw  accuracy: 0.97990396\n",
            " -- 3.1705 bpw  accuracy: 0.98084363\n",
            " -- 4.0439 bpw  accuracy: 0.98534533\n",
            " -- 4.0471 bpw  accuracy: 0.98675803\n",
            " -- 4.0816 bpw  accuracy: 0.98801213\n",
            " -- 4.1381 bpw  accuracy: 0.98836467\n",
            " -- 4.1705 bpw  accuracy: 0.99026544\n",
            " -- 4.1902 bpw  accuracy: 0.99094024\n",
            " -- 4.2737 bpw  accuracy: 0.99156918\n",
            " -- 4.3295 bpw  accuracy: 0.99229064\n",
            " -- 5.2564 bpw  accuracy: 0.99493717\n",
            " -- 5.3295 bpw  accuracy: 0.99613090\n",
            " -- 6.0439 bpw  accuracy: 0.99582996\n",
            " -- 6.3381 bpw  accuracy: 0.99796351\n",
            " -- 8.0439 bpw  accuracy: 0.99890425\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.25 (Attention)        |\n",
            "| Duration: 12.06 seconds                      |\n",
            "| Completed step: 51/67                        |\n",
            "| Avg time / step (rolling): 22.99 seconds     |\n",
            "| Estimated remaining time: 6min 7sec          |\n",
            "| Last checkpoint layer: model.layers.23 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.25 (MLP)\n",
            " -- model.layers.25.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.25.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.25.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.25.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.25.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.25.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.25.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.25.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91408373\n",
            " -- 2.3230 bpw  accuracy: 0.91658161\n",
            " -- 2.5958 bpw  accuracy: 0.92834551\n",
            " -- 2.9120 bpw  accuracy: 0.93212452\n",
            " -- 3.2833 bpw  accuracy: 0.95646317\n",
            " -- 3.3655 bpw  accuracy: 0.95986035\n",
            " -- 3.6186 bpw  accuracy: 0.96498522\n",
            " -- 4.1368 bpw  accuracy: 0.97706547\n",
            " -- 4.1977 bpw  accuracy: 0.97908159\n",
            " -- 4.2662 bpw  accuracy: 0.97785401\n",
            " -- 4.3484 bpw  accuracy: 0.98055461\n",
            " -- 5.2491 bpw  accuracy: 0.98874060\n",
            " -- 5.3313 bpw  accuracy: 0.99031881\n",
            " -- 6.0713 bpw  accuracy: 0.99380535\n",
            " -- 6.3032 bpw  accuracy: 0.99429043\n",
            " -- 6.8687 bpw  accuracy: 0.99529690\n",
            " -- 8.0354 bpw  accuracy: 0.99829267\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.25 (MLP)              |\n",
            "| Duration: 34.05 seconds                      |\n",
            "| Completed step: 52/67                        |\n",
            "| Avg time / step (rolling): 23.01 seconds     |\n",
            "| Estimated remaining time: 5min 45sec         |\n",
            "| Last checkpoint layer: model.layers.23 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.26 (Attention)\n",
            " -- model.layers.26.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.26.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.26.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.26.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.26.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.96226496\n",
            " -- 2.1987 bpw  accuracy: 0.96374796\n",
            " -- 2.2831 bpw  accuracy: 0.96728361\n",
            " -- 2.6768 bpw  accuracy: 0.97462485\n",
            " -- 3.1689 bpw  accuracy: 0.97966360\n",
            " -- 3.1705 bpw  accuracy: 0.98046957\n",
            " -- 4.0439 bpw  accuracy: 0.98569630\n",
            " -- 4.0471 bpw  accuracy: 0.98663128\n",
            " -- 4.0816 bpw  accuracy: 0.98772842\n",
            " -- 4.1381 bpw  accuracy: 0.98780873\n",
            " -- 4.1705 bpw  accuracy: 0.99015509\n",
            " -- 4.1902 bpw  accuracy: 0.99067327\n",
            " -- 4.2737 bpw  accuracy: 0.99132762\n",
            " -- 4.3295 bpw  accuracy: 0.99193709\n",
            " -- 5.2564 bpw  accuracy: 0.99476430\n",
            " -- 5.3295 bpw  accuracy: 0.99600561\n",
            " -- 6.0439 bpw  accuracy: 0.99574439\n",
            " -- 6.3381 bpw  accuracy: 0.99789062\n",
            " -- 8.0439 bpw  accuracy: 0.99886594\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.26 (Attention)        |\n",
            "| Duration: 11.88 seconds                      |\n",
            "| Completed step: 53/67                        |\n",
            "| Avg time / step (rolling): 23.00 seconds     |\n",
            "| Estimated remaining time: 5min 22sec         |\n",
            "| Last checkpoint layer: model.layers.23 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.26 (MLP)\n",
            " -- model.layers.26.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.26.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.26.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.26.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.26.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.26.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.26.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.26.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91220704\n",
            " -- 2.3230 bpw  accuracy: 0.91475706\n",
            " -- 2.5958 bpw  accuracy: 0.92648144\n",
            " -- 2.9120 bpw  accuracy: 0.93029952\n",
            " -- 3.2833 bpw  accuracy: 0.95540529\n",
            " -- 3.3655 bpw  accuracy: 0.95892851\n",
            " -- 3.6186 bpw  accuracy: 0.96407402\n",
            " -- 4.1368 bpw  accuracy: 0.97650182\n",
            " -- 4.1977 bpw  accuracy: 0.97855930\n",
            " -- 4.2662 bpw  accuracy: 0.97730471\n",
            " -- 4.3484 bpw  accuracy: 0.98009800\n",
            " -- 5.2491 bpw  accuracy: 0.98845722\n",
            " -- 5.3313 bpw  accuracy: 0.99008362\n",
            " -- 6.0713 bpw  accuracy: 0.99363205\n",
            " -- 6.3032 bpw  accuracy: 0.99414638\n",
            " -- 6.8687 bpw  accuracy: 0.99515435\n",
            " -- 8.0354 bpw  accuracy: 0.99821012\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.26 (MLP)              |\n",
            "| Duration: 33.86 seconds                      |\n",
            "| Completed step: 54/67                        |\n",
            "| Avg time / step (rolling): 22.98 seconds     |\n",
            "| Estimated remaining time: 4min 58sec         |\n",
            "| Last checkpoint layer: model.layers.23 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.27 (Attention)\n",
            " -- model.layers.27.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.27.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.27.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.27.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.27.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.96160406\n",
            " -- 2.1987 bpw  accuracy: 0.96318289\n",
            " -- 2.2831 bpw  accuracy: 0.96765929\n",
            " -- 2.6768 bpw  accuracy: 0.97472490\n",
            " -- 3.1689 bpw  accuracy: 0.97987545\n",
            " -- 3.1705 bpw  accuracy: 0.98059758\n",
            " -- 4.0439 bpw  accuracy: 0.98593180\n",
            " -- 4.0471 bpw  accuracy: 0.98691583\n",
            " -- 4.0816 bpw  accuracy: 0.98784235\n",
            " -- 4.1381 bpw  accuracy: 0.98856410\n",
            " -- 4.1705 bpw  accuracy: 0.99007280\n",
            " -- 4.1902 bpw  accuracy: 0.99071385\n",
            " -- 4.2737 bpw  accuracy: 0.99124282\n",
            " -- 4.3295 bpw  accuracy: 0.99189155\n",
            " -- 5.2564 bpw  accuracy: 0.99500938\n",
            " -- 5.3295 bpw  accuracy: 0.99599362\n",
            " -- 6.0439 bpw  accuracy: 0.99611215\n",
            " -- 6.3381 bpw  accuracy: 0.99792379\n",
            " -- 8.0439 bpw  accuracy: 0.99894116\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.27 (Attention)        |\n",
            "| Duration: 12.05 seconds                      |\n",
            "| Completed step: 55/67                        |\n",
            "| Avg time / step (rolling): 22.99 seconds     |\n",
            "| Estimated remaining time: 4min 35sec         |\n",
            "| Last checkpoint layer: model.layers.23 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.27 (MLP)\n",
            " -- model.layers.27.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.27.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.27.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.27.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.27.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.27.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.27.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.27.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.91032989\n",
            " -- 2.3230 bpw  accuracy: 0.91298755\n",
            " -- 2.5958 bpw  accuracy: 0.92489017\n",
            " -- 2.9120 bpw  accuracy: 0.92879948\n",
            " -- 3.2833 bpw  accuracy: 0.95435773\n",
            " -- 3.3655 bpw  accuracy: 0.95794147\n",
            " -- 3.6186 bpw  accuracy: 0.96321213\n",
            " -- 4.1368 bpw  accuracy: 0.97585048\n",
            " -- 4.1977 bpw  accuracy: 0.97798234\n",
            " -- 4.2662 bpw  accuracy: 0.97677132\n",
            " -- 4.3484 bpw  accuracy: 0.97962346\n",
            " -- 5.2491 bpw  accuracy: 0.98819851\n",
            " -- 5.3313 bpw  accuracy: 0.98984934\n",
            " -- 6.0713 bpw  accuracy: 0.99345932\n",
            " -- 6.3032 bpw  accuracy: 0.99401268\n",
            " -- 6.8687 bpw  accuracy: 0.99504861\n",
            " -- 8.0354 bpw  accuracy: 0.99814431\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.27 (MLP)              |\n",
            "| Duration: 33.97 seconds                      |\n",
            "| Completed step: 56/67                        |\n",
            "| Avg time / step (rolling): 22.98 seconds     |\n",
            "| Estimated remaining time: 4min 12sec         |\n",
            "| Last checkpoint layer: model.layers.23 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.28 (Attention)\n",
            " -- model.layers.28.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.28.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.28.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.28.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.28.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.95566610\n",
            " -- 2.1987 bpw  accuracy: 0.95779977\n",
            " -- 2.2831 bpw  accuracy: 0.96239198\n",
            " -- 2.6768 bpw  accuracy: 0.97110887\n",
            " -- 3.1689 bpw  accuracy: 0.97689971\n",
            " -- 3.1705 bpw  accuracy: 0.97764647\n",
            " -- 4.0439 bpw  accuracy: 0.98388138\n",
            " -- 4.0471 bpw  accuracy: 0.98485097\n",
            " -- 4.0816 bpw  accuracy: 0.98628805\n",
            " -- 4.1381 bpw  accuracy: 0.98688308\n",
            " -- 4.1705 bpw  accuracy: 0.98863484\n",
            " -- 4.1902 bpw  accuracy: 0.98942486\n",
            " -- 4.2737 bpw  accuracy: 0.99008797\n",
            " -- 4.3295 bpw  accuracy: 0.99090450\n",
            " -- 5.2564 bpw  accuracy: 0.99428032\n",
            " -- 5.3295 bpw  accuracy: 0.99537329\n",
            " -- 6.0439 bpw  accuracy: 0.99550669\n",
            " -- 6.3381 bpw  accuracy: 0.99745212\n",
            " -- 8.0439 bpw  accuracy: 0.99875961\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.28 (Attention)        |\n",
            "| Duration: 11.94 seconds                      |\n",
            "| Completed step: 57/67                        |\n",
            "| Avg time / step (rolling): 22.97 seconds     |\n",
            "| Estimated remaining time: 3min 49sec         |\n",
            "| Last checkpoint layer: model.layers.27 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.28 (MLP)\n",
            " -- model.layers.28.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.28.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.28.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.28.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.28.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.28.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.28.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.28.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.90378511\n",
            " -- 2.3230 bpw  accuracy: 0.90666246\n",
            " -- 2.5958 bpw  accuracy: 0.91989426\n",
            " -- 2.9120 bpw  accuracy: 0.92431042\n",
            " -- 3.2833 bpw  accuracy: 0.95098403\n",
            " -- 3.3655 bpw  accuracy: 0.95486298\n",
            " -- 3.6186 bpw  accuracy: 0.96074617\n",
            " -- 4.1368 bpw  accuracy: 0.97390766\n",
            " -- 4.1977 bpw  accuracy: 0.97622691\n",
            " -- 4.2662 bpw  accuracy: 0.97503045\n",
            " -- 4.3484 bpw  accuracy: 0.97811373\n",
            " -- 5.2491 bpw  accuracy: 0.98730669\n",
            " -- 5.3313 bpw  accuracy: 0.98908933\n",
            " -- 6.0713 bpw  accuracy: 0.99290057\n",
            " -- 6.3032 bpw  accuracy: 0.99357856\n",
            " -- 6.8687 bpw  accuracy: 0.99475442\n",
            " -- 8.0354 bpw  accuracy: 0.99800259\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.28 (MLP)              |\n",
            "| Duration: 34.02 seconds                      |\n",
            "| Completed step: 58/67                        |\n",
            "| Avg time / step (rolling): 22.97 seconds     |\n",
            "| Estimated remaining time: 3min 26sec         |\n",
            "| Last checkpoint layer: model.layers.27 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.29 (Attention)\n",
            " -- model.layers.29.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.29.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.29.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.29.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.29.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94369422\n",
            " -- 2.1987 bpw  accuracy: 0.94609539\n",
            " -- 2.2831 bpw  accuracy: 0.95318407\n",
            " -- 2.6768 bpw  accuracy: 0.96340803\n",
            " -- 3.1689 bpw  accuracy: 0.97089370\n",
            " -- 3.1705 bpw  accuracy: 0.97151723\n",
            " -- 4.0439 bpw  accuracy: 0.98022538\n",
            " -- 4.0471 bpw  accuracy: 0.98100285\n",
            " -- 4.0816 bpw  accuracy: 0.98245847\n",
            " -- 4.1381 bpw  accuracy: 0.98348722\n",
            " -- 4.1705 bpw  accuracy: 0.98524744\n",
            " -- 4.1902 bpw  accuracy: 0.98633939\n",
            " -- 4.2737 bpw  accuracy: 0.98746550\n",
            " -- 4.3295 bpw  accuracy: 0.98837066\n",
            " -- 5.2564 bpw  accuracy: 0.99300754\n",
            " -- 5.3295 bpw  accuracy: 0.99417307\n",
            " -- 6.0439 bpw  accuracy: 0.99456764\n",
            " -- 6.3381 bpw  accuracy: 0.99701945\n",
            " -- 8.0439 bpw  accuracy: 0.99855181\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.29 (Attention)        |\n",
            "| Duration: 12.04 seconds                      |\n",
            "| Completed step: 59/67                        |\n",
            "| Avg time / step (rolling): 22.98 seconds     |\n",
            "| Estimated remaining time: 3min 3sec          |\n",
            "| Last checkpoint layer: model.layers.27 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.29 (MLP)\n",
            " -- model.layers.29.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.29.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.29.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.29.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.29.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.29.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.29.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.29.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.90313367\n",
            " -- 2.3230 bpw  accuracy: 0.90606377\n",
            " -- 2.5958 bpw  accuracy: 0.91966039\n",
            " -- 2.9120 bpw  accuracy: 0.92417615\n",
            " -- 3.2833 bpw  accuracy: 0.95051413\n",
            " -- 3.3655 bpw  accuracy: 0.95439962\n",
            " -- 3.6186 bpw  accuracy: 0.96048317\n",
            " -- 4.1368 bpw  accuracy: 0.97343511\n",
            " -- 4.1977 bpw  accuracy: 0.97576757\n",
            " -- 4.2662 bpw  accuracy: 0.97464513\n",
            " -- 4.3484 bpw  accuracy: 0.97765558\n",
            " -- 5.2491 bpw  accuracy: 0.98703892\n",
            " -- 5.3313 bpw  accuracy: 0.98882174\n",
            " -- 6.0713 bpw  accuracy: 0.99276578\n",
            " -- 6.3032 bpw  accuracy: 0.99340849\n",
            " -- 6.8687 bpw  accuracy: 0.99460557\n",
            " -- 8.0354 bpw  accuracy: 0.99800954\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.29 (MLP)              |\n",
            "| Duration: 34.06 seconds                      |\n",
            "| Completed step: 60/67                        |\n",
            "| Avg time / step (rolling): 23.00 seconds     |\n",
            "| Estimated remaining time: 2min 40sec         |\n",
            "| Last checkpoint layer: model.layers.27 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.30 (Attention)\n",
            " -- model.layers.30.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.30.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.30.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.30.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.30.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.94085100\n",
            " -- 2.1987 bpw  accuracy: 0.94362952\n",
            " -- 2.2831 bpw  accuracy: 0.95185889\n",
            " -- 2.6768 bpw  accuracy: 0.96062800\n",
            " -- 3.1689 bpw  accuracy: 0.96788531\n",
            " -- 3.1705 bpw  accuracy: 0.97090238\n",
            " -- 4.0439 bpw  accuracy: 0.97470775\n",
            " -- 4.0471 bpw  accuracy: 0.97797868\n",
            " -- 4.0816 bpw  accuracy: 0.97881716\n",
            " -- 4.1381 bpw  accuracy: 0.97939369\n",
            " -- 4.1705 bpw  accuracy: 0.98599288\n",
            " -- 4.1902 bpw  accuracy: 0.98683970\n",
            " -- 4.2737 bpw  accuracy: 0.98765240\n",
            " -- 4.3295 bpw  accuracy: 0.98852310\n",
            " -- 5.2564 bpw  accuracy: 0.99298498\n",
            " -- 5.3295 bpw  accuracy: 0.99412648\n",
            " -- 6.0439 bpw  accuracy: 0.99433308\n",
            " -- 6.3381 bpw  accuracy: 0.99694584\n",
            " -- 8.0439 bpw  accuracy: 0.99843384\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.30 (Attention)        |\n",
            "| Duration: 11.84 seconds                      |\n",
            "| Completed step: 61/67                        |\n",
            "| Avg time / step (rolling): 22.97 seconds     |\n",
            "| Estimated remaining time: 2min 17sec         |\n",
            "| Last checkpoint layer: model.layers.27 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.30 (MLP)\n",
            " -- model.layers.30.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.30.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.30.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.30.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.30.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.30.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.30.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.30.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.89440048\n",
            " -- 2.3230 bpw  accuracy: 0.89717557\n",
            " -- 2.5958 bpw  accuracy: 0.91156459\n",
            " -- 2.9120 bpw  accuracy: 0.91619827\n",
            " -- 3.2833 bpw  accuracy: 0.94636323\n",
            " -- 3.3655 bpw  accuracy: 0.95072527\n",
            " -- 3.6186 bpw  accuracy: 0.95721954\n",
            " -- 4.1368 bpw  accuracy: 0.97073510\n",
            " -- 4.1977 bpw  accuracy: 0.97355560\n",
            " -- 4.2662 bpw  accuracy: 0.97230197\n",
            " -- 4.3484 bpw  accuracy: 0.97564030\n",
            " -- 5.2491 bpw  accuracy: 0.98565681\n",
            " -- 5.3313 bpw  accuracy: 0.98775256\n",
            " -- 6.0713 bpw  accuracy: 0.99186824\n",
            " -- 6.3032 bpw  accuracy: 0.99270174\n",
            " -- 6.8687 bpw  accuracy: 0.99411882\n",
            " -- 8.0354 bpw  accuracy: 0.99773410\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.30 (MLP)              |\n",
            "| Duration: 33.91 seconds                      |\n",
            "| Completed step: 62/67                        |\n",
            "| Avg time / step (rolling): 22.96 seconds     |\n",
            "| Estimated remaining time: 1min 54sec         |\n",
            "| Last checkpoint layer: model.layers.27 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.31 (Attention)\n",
            " -- model.layers.31.self_attn.q_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.31.self_attn.q_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.20 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.19 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.19 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.26 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.31.self_attn.k_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.16 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   0.25:3b_64g/0.75:2b_64g s4                         2.34 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.19 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   0.1:4b_64g/0.9:3b_64g s4                           3.20 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:4b_128g s4                                       4.06 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:4b_64g s4                                        4.09 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:4b_32g s4                                        4.16 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.20 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.26 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:5b_64g s4                                        5.09 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:5b_32g s4                                        5.16 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:6b_128g s4                                       6.06 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:6b_32g s4                                        6.16 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:8b_32g s4                                        8.16 bpw\n",
            " -- model.layers.31.self_attn.v_proj                   1:8b_128g s4                                       8.06 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:3b_64g/0.9:2b_64g s4                           2.18 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   1:4b_128g s4                                       4.04 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   1:4b_64g s4                                        4.07 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:5b_64g/0.9:4b_64g s4                           4.18 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   1:6b_128g s4                                       6.04 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   1:6b_32g s4                                        6.13 bpw\n",
            " -- model.layers.31.self_attn.o_proj                   1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.1378 bpw  accuracy: 0.93302331\n",
            " -- 2.1987 bpw  accuracy: 0.93534842\n",
            " -- 2.2831 bpw  accuracy: 0.94564187\n",
            " -- 2.6768 bpw  accuracy: 0.95836522\n",
            " -- 3.1689 bpw  accuracy: 0.97124833\n",
            " -- 3.1705 bpw  accuracy: 0.97196335\n",
            " -- 4.0439 bpw  accuracy: 0.98044352\n",
            " -- 4.0471 bpw  accuracy: 0.98173889\n",
            " -- 4.0816 bpw  accuracy: 0.98426918\n",
            " -- 4.1381 bpw  accuracy: 0.98494215\n",
            " -- 4.1705 bpw  accuracy: 0.98609338\n",
            " -- 4.1902 bpw  accuracy: 0.98699982\n",
            " -- 4.2737 bpw  accuracy: 0.98821953\n",
            " -- 4.3295 bpw  accuracy: 0.98907104\n",
            " -- 5.2564 bpw  accuracy: 0.99348937\n",
            " -- 5.3295 bpw  accuracy: 0.99435629\n",
            " -- 6.0439 bpw  accuracy: 0.99523822\n",
            " -- 6.3381 bpw  accuracy: 0.99697390\n",
            " -- 8.0439 bpw  accuracy: 0.99865403\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.31 (Attention)        |\n",
            "| Duration: 11.95 seconds                      |\n",
            "| Completed step: 63/67                        |\n",
            "| Avg time / step (rolling): 22.97 seconds     |\n",
            "| Estimated remaining time: 1min 31sec         |\n",
            "| Last checkpoint layer: model.layers.27 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: model.layers.31 (MLP)\n",
            " -- model.layers.31.mlp.gate_proj                      0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:3b_64g/0.9:2b_64g s4                           2.17 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:4b_128g/0.9:3b_128g s4                         3.16 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:4b_32g/0.9:3b_32g s4                           3.23 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      1:4b_128g s4                                       4.03 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:5b_128g/0.9:4b_128g s4                         4.16 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:5b_32g/0.9:4b_32g s4                           4.23 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:6b_128g/0.9:5b_128g s4                         5.16 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:6b_32g/0.9:5b_32g s4                           5.23 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.31.mlp.gate_proj                      1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.05:3b_64g/0.95:2b_64g s4                         2.13 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:3b_64g/0.75:2b_64g s4                         2.31 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.3:3b_64g/0.7:2b_64g s4                           2.38 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:4b_128g/0.75:3b_128g s4                       3.28 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:4b_32g/0.75:3b_32g s4                         3.38 bpw\n",
            " -- model.layers.31.mlp.up_proj                        1:4b_32g s4                                        4.13 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:5b_128g/0.75:4b_128g s4                       4.28 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:5b_32g/0.75:4b_32g s4                         4.38 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:6b_128g/0.75:5b_128g s4                       5.28 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.25:6b_32g/0.75:5b_32g s4                         5.38 bpw\n",
            " -- model.layers.31.mlp.up_proj                        1:6b_128g s4                                       6.03 bpw\n",
            " -- model.layers.31.mlp.up_proj                        0.1:8b_128g/0.9:6b_128g s4                         6.28 bpw\n",
            " -- model.layers.31.mlp.up_proj                        1:8b_128g s4                                       8.03 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4              2.48 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:5b_32g/0.95:3b_32g s4                         3.24 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:5b_32g/0.95:4b_32g s4                         4.18 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.1:4b_128g/0.85:3b_128g s4            3.41 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4              3.49 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.95:4b_128g s4                        4.25 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.95:4b_32g s4                         4.34 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4            4.36 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4              4.44 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.1:6b_128g/0.85:5b_128g s4            5.31 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.1:6b_32g/0.85:5b_32g s4              5.39 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.05:8b_32g/0.95:6b_128g s4                        6.15 bpw\n",
            " -- model.layers.31.mlp.down_proj                      0.15:8b_128g/0.85:6b_128g s4                       6.34 bpw\n",
            " -- model.layers.31.mlp.down_proj                      1:8b_128g s4                                       8.04 bpw\n",
            " -- 2.2449 bpw  accuracy: 0.89348449\n",
            " -- 2.3230 bpw  accuracy: 0.89637273\n",
            " -- 2.5958 bpw  accuracy: 0.91020142\n",
            " -- 2.9120 bpw  accuracy: 0.91476823\n",
            " -- 3.2833 bpw  accuracy: 0.94536082\n",
            " -- 3.3655 bpw  accuracy: 0.95022537\n",
            " -- 3.6186 bpw  accuracy: 0.95641279\n",
            " -- 4.1368 bpw  accuracy: 0.97063887\n",
            " -- 4.1977 bpw  accuracy: 0.97365729\n",
            " -- 4.2662 bpw  accuracy: 0.97190128\n",
            " -- 4.3484 bpw  accuracy: 0.97561462\n",
            " -- 5.2491 bpw  accuracy: 0.98552038\n",
            " -- 5.3313 bpw  accuracy: 0.98772911\n",
            " -- 6.0713 bpw  accuracy: 0.99174033\n",
            " -- 6.3032 bpw  accuracy: 0.99259526\n",
            " -- 6.8687 bpw  accuracy: 0.99397933\n",
            " -- 8.0354 bpw  accuracy: 0.99742786\n",
            "------------------------------------------------\n",
            "| Measured: model.layers.31 (MLP)              |\n",
            "| Duration: 33.79 seconds                      |\n",
            "| Completed step: 64/67                        |\n",
            "| Avg time / step (rolling): 22.96 seconds     |\n",
            "| Estimated remaining time: 1min 8sec          |\n",
            "| Last checkpoint layer: model.layers.27 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.norm (RMSNorm)\n",
            "------------------------------------------------\n",
            "| Measured: model.norm (RMSNorm)               |\n",
            "| Duration: 0.28 seconds                       |\n",
            "| Completed step: 65/67                        |\n",
            "| Avg time / step (rolling): 21.78 seconds     |\n",
            "| Estimated remaining time: 0min 43sec         |\n",
            "| Last checkpoint layer: model.layers.31 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Layer: lm_head (Linear)\n",
            "------------------------------------------------\n",
            "| Measured: lm_head (Linear)                   |\n",
            "| Duration: 1.36 seconds                       |\n",
            "| Completed step: 66/67                        |\n",
            "| Avg time / step (rolling): 18.52 seconds     |\n",
            "| Estimated remaining time: 0min 18sec         |\n",
            "| Last checkpoint layer: model.layers.31 (MLP) |\n",
            "------------------------------------------------\n",
            " -- Saving checkpoint...\n",
            " -- Optimizing...\n",
            " -- Optimizing:    1/ 240\n",
            " -- Optimizing:    8/ 240\n",
            " -- Optimizing:   15/ 240\n",
            " -- Optimizing:   22/ 240\n",
            " -- Optimizing:   29/ 240\n",
            " -- Optimizing:   36/ 240\n",
            " -- Optimizing:   43/ 240\n",
            " -- Optimizing:   50/ 240\n",
            " -- Optimizing:   57/ 240\n",
            " -- Optimizing:   64/ 240\n",
            " -- Optimizing:   71/ 240\n",
            " -- Optimizing:   78/ 240\n",
            " -- Optimizing:   80/ 240\n",
            " -- Optimizing:   87/ 240\n",
            " -- Optimizing:   94/ 240\n",
            " -- Optimizing:  101/ 240\n",
            " -- Optimizing:  108/ 240\n",
            " -- Optimizing:  115/ 240\n",
            " -- Optimizing:  122/ 240\n",
            " -- Optimizing:  129/ 240\n",
            " -- Optimizing:  136/ 240\n",
            " -- Optimizing:  143/ 240\n",
            " -- Optimizing:  150/ 240\n",
            " -- Optimizing:  157/ 240\n",
            " -- Optimizing:  164/ 240\n",
            " -- Optimizing:  171/ 240\n",
            " -- Optimizing:  178/ 240\n",
            " -- Optimizing:  185/ 240\n",
            " -- Optimizing:  192/ 240\n",
            " -- Optimizing:  199/ 240\n",
            " -- Optimizing:  206/ 240\n",
            " -- Optimizing:  213/ 240\n",
            " -- Optimizing:  220/ 240\n",
            " -- Optimizing:  227/ 240\n",
            " -- Optimizing:  234/ 240\n",
            " -- max(err): 0.028099\n",
            " -- error_norm: 1.905063\n",
            " -- Quantization strategy:\n",
            " --   model.layers.0.self_attn                           5.3295 bpw - exp. error: 0.01243892\n",
            " --   model.layers.0.mlp                                 4.1977 bpw - exp. error: 0.02316471\n",
            " --   model.layers.1.self_attn                           6.3381 bpw - exp. error: 0.00675082\n",
            " --   model.layers.1.mlp                                 3.3655 bpw - exp. error: 0.01386648\n",
            " --   model.layers.2.self_attn                           3.1689 bpw - exp. error: 0.00898621\n",
            " --   model.layers.2.mlp                                 2.2449 bpw - exp. error: 0.02217232\n",
            " --   model.layers.3.self_attn                           2.6768 bpw - exp. error: 0.01153749\n",
            " --   model.layers.3.mlp                                 2.3230 bpw - exp. error: 0.02595236\n",
            " --   model.layers.4.self_attn                           2.1378 bpw - exp. error: 0.01747415\n",
            " --   model.layers.4.mlp                                 2.5958 bpw - exp. error: 0.02451262\n",
            " --   model.layers.5.self_attn                           3.1705 bpw - exp. error: 0.01162412\n",
            " --   model.layers.5.mlp                                 3.3655 bpw - exp. error: 0.01657775\n",
            " --   model.layers.6.self_attn                           4.2737 bpw - exp. error: 0.00533969\n",
            " --   model.layers.6.mlp                                 3.3655 bpw - exp. error: 0.01831759\n",
            " --   model.layers.7.self_attn                           4.0439 bpw - exp. error: 0.00968624\n",
            " --   model.layers.7.mlp                                 3.6186 bpw - exp. error: 0.01697350\n",
            " --   model.layers.8.self_attn                           4.0471 bpw - exp. error: 0.01017824\n",
            " --   model.layers.8.mlp                                 3.3655 bpw - exp. error: 0.02097592\n",
            " --   model.layers.9.self_attn                           5.2564 bpw - exp. error: 0.00361262\n",
            " --   model.layers.9.mlp                                 4.3484 bpw - exp. error: 0.01076520\n",
            " --   model.layers.10.self_attn                          4.1902 bpw - exp. error: 0.00835164\n",
            " --   model.layers.10.mlp                                4.1977 bpw - exp. error: 0.01188652\n",
            " --   model.layers.11.self_attn                          4.1381 bpw - exp. error: 0.01156505\n",
            " --   model.layers.11.mlp                                3.3655 bpw - exp. error: 0.02347162\n",
            " --   model.layers.12.self_attn                          4.1705 bpw - exp. error: 0.01163896\n",
            " --   model.layers.12.mlp                                4.1368 bpw - exp. error: 0.01476438\n",
            " --   model.layers.13.self_attn                          5.3295 bpw - exp. error: 0.00435268\n",
            " --   model.layers.13.mlp                                4.2662 bpw - exp. error: 0.01526959\n",
            " --   model.layers.14.self_attn                          4.1381 bpw - exp. error: 0.01377695\n",
            " --   model.layers.14.mlp                                4.1977 bpw - exp. error: 0.01541692\n",
            " --   model.layers.15.self_attn                          4.2737 bpw - exp. error: 0.01253850\n",
            " --   model.layers.15.mlp                                4.1368 bpw - exp. error: 0.01804334\n",
            " --   model.layers.16.self_attn                          4.3295 bpw - exp. error: 0.01006202\n",
            " --   model.layers.16.mlp                                4.1977 bpw - exp. error: 0.01800463\n",
            " --   model.layers.17.self_attn                          4.2737 bpw - exp. error: 0.01224011\n",
            " --   model.layers.17.mlp                                4.2662 bpw - exp. error: 0.02064065\n",
            " --   model.layers.18.self_attn                          4.3295 bpw - exp. error: 0.01105552\n",
            " --   model.layers.18.mlp                                4.1977 bpw - exp. error: 0.02116143\n",
            " --   model.layers.19.self_attn                          4.1705 bpw - exp. error: 0.01309177\n",
            " --   model.layers.19.mlp                                4.3484 bpw - exp. error: 0.01956300\n",
            " --   model.layers.20.self_attn                          4.3295 bpw - exp. error: 0.00982642\n",
            " --   model.layers.20.mlp                                4.1368 bpw - exp. error: 0.02326088\n",
            " --   model.layers.21.self_attn                          4.2737 bpw - exp. error: 0.00991228\n",
            " --   model.layers.21.mlp                                4.1977 bpw - exp. error: 0.02066668\n",
            " --   model.layers.22.self_attn                          5.2564 bpw - exp. error: 0.00524883\n",
            " --   model.layers.22.mlp                                4.1977 bpw - exp. error: 0.02050188\n",
            " --   model.layers.23.self_attn                          4.3295 bpw - exp. error: 0.00809134\n",
            " --   model.layers.23.mlp                                4.1977 bpw - exp. error: 0.02062794\n",
            " --   model.layers.24.self_attn                          5.2564 bpw - exp. error: 0.00486502\n",
            " --   model.layers.24.mlp                                4.3484 bpw - exp. error: 0.01925263\n",
            " --   model.layers.25.self_attn                          4.2737 bpw - exp. error: 0.00843082\n",
            " --   model.layers.25.mlp                                4.3484 bpw - exp. error: 0.01944539\n",
            " --   model.layers.26.self_attn                          4.1705 bpw - exp. error: 0.00984491\n",
            " --   model.layers.26.mlp                                4.1977 bpw - exp. error: 0.02144070\n",
            " --   model.layers.27.self_attn                          4.1705 bpw - exp. error: 0.00992720\n",
            " --   model.layers.27.mlp                                4.1977 bpw - exp. error: 0.02201766\n",
            " --   model.layers.28.self_attn                          4.3295 bpw - exp. error: 0.00909550\n",
            " --   model.layers.28.mlp                                4.1977 bpw - exp. error: 0.02377309\n",
            " --   model.layers.29.self_attn                          5.2564 bpw - exp. error: 0.00699246\n",
            " --   model.layers.29.mlp                                4.3484 bpw - exp. error: 0.02234442\n",
            " --   model.layers.30.self_attn                          6.0439 bpw - exp. error: 0.00566692\n",
            " --   model.layers.30.mlp                                4.1977 bpw - exp. error: 0.02644440\n",
            " --   model.layers.31.self_attn                          5.2564 bpw - exp. error: 0.00651063\n",
            " --   model.layers.31.mlp                                4.2662 bpw - exp. error: 0.02809872\n",
            " -- sum(log(err)): -277.234195\n",
            " -- max(err): 0.028099\n",
            " -- Tokenizing samples...\n",
            " -- Token embeddings again...\n",
            " -- Quantizing...\n",
            " -- Layer: model.layers.0 (Attention)\n",
            " -- Linear: model.layers.0.self_attn.q_proj -> 0.1:6b_32g/0.9:5b_32g s4, 5.23 bpw\n",
            " -- Linear: model.layers.0.self_attn.k_proj -> 0.1:6b_32g/0.9:5b_32g s4, 5.26 bpw\n",
            " -- Linear: model.layers.0.self_attn.v_proj -> 1:6b_32g s4, 6.16 bpw\n",
            " -- Linear: model.layers.0.self_attn.o_proj -> 0.1:6b_32g/0.9:5b_32g s4, 5.23 bpw\n",
            " -- Module quantized, rfn_error: 0.014221\n",
            " -- Layer: model.layers.0 (MLP)\n",
            " -- Linear: model.layers.0.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.0.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.0.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.027821\n",
            " -- Layer: model.layers.1 (Attention)\n",
            " -- Linear: model.layers.1.self_attn.q_proj -> 1:6b_32g s4, 6.13 bpw\n",
            " -- Linear: model.layers.1.self_attn.k_proj -> 1:6b_32g s4, 6.16 bpw\n",
            " -- Linear: model.layers.1.self_attn.v_proj -> 1:8b_32g s4, 8.16 bpw\n",
            " -- Linear: model.layers.1.self_attn.o_proj -> 1:6b_32g s4, 6.13 bpw\n",
            " -- Module quantized, rfn_error: 0.007050\n",
            " -- Layer: model.layers.1 (MLP)\n",
            " -- Linear: model.layers.1.mlp.gate_proj -> 0.1:4b_32g/0.9:3b_32g s4, 3.23 bpw\n",
            " -- Linear: model.layers.1.mlp.up_proj -> 0.25:4b_32g/0.75:3b_32g s4, 3.38 bpw\n",
            " -- Linear: model.layers.1.mlp.down_proj -> 0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4, 3.49 bpw\n",
            " -- Module quantized, rfn_error: 0.007409\n",
            " -- Layer: model.layers.2 (Attention)\n",
            " -- Linear: model.layers.2.self_attn.q_proj -> 0.1:4b_128g/0.9:3b_128g s4, 3.16 bpw\n",
            " -- Linear: model.layers.2.self_attn.k_proj -> 0.1:4b_128g/0.9:3b_128g s4, 3.19 bpw\n",
            " -- Linear: model.layers.2.self_attn.v_proj -> 0.1:4b_128g/0.9:3b_128g s4, 3.19 bpw\n",
            " -- Linear: model.layers.2.self_attn.o_proj -> 0.1:4b_128g/0.9:3b_128g s4, 3.16 bpw\n",
            " -- Module quantized, rfn_error: 0.004401\n",
            " -- Layer: model.layers.2 (MLP)\n",
            " -- Linear: model.layers.2.mlp.gate_proj -> 0.05:3b_64g/0.95:2b_64g s4, 2.13 bpw\n",
            " -- Linear: model.layers.2.mlp.up_proj -> 0.05:3b_64g/0.95:2b_64g s4, 2.13 bpw\n",
            " -- Linear: model.layers.2.mlp.down_proj -> 0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4, 2.48 bpw\n",
            " -- Module quantized, rfn_error: 0.012257\n",
            " -- Layer: model.layers.3 (Attention)\n",
            " -- Linear: model.layers.3.self_attn.q_proj -> 0.1:3b_64g/0.9:2b_64g s4, 2.18 bpw\n",
            " -- Linear: model.layers.3.self_attn.k_proj -> 0.1:3b_64g/0.9:2b_64g s4, 2.20 bpw\n",
            " -- Linear: model.layers.3.self_attn.v_proj -> 0.1:4b_128g/0.9:3b_128g s4, 3.19 bpw\n",
            " -- Linear: model.layers.3.self_attn.o_proj -> 0.1:4b_128g/0.9:3b_128g s4, 3.16 bpw\n",
            " -- Module quantized, rfn_error: 0.005541\n",
            " -- Layer: model.layers.3 (MLP)\n",
            " -- Linear: model.layers.3.mlp.gate_proj -> 0.1:3b_64g/0.9:2b_64g s4, 2.17 bpw\n",
            " -- Linear: model.layers.3.mlp.up_proj -> 0.25:3b_64g/0.75:2b_64g s4, 2.31 bpw\n",
            " -- Linear: model.layers.3.mlp.down_proj -> 0.05:6b_32g/0.2:3b_64g/0.75:2b_64g s4, 2.48 bpw\n",
            " -- Module quantized, rfn_error: 0.015681\n",
            " -- Layer: model.layers.4 (Attention)\n",
            " -- Linear: model.layers.4.self_attn.q_proj -> 0.05:3b_64g/0.95:2b_64g s4, 2.13 bpw\n",
            " -- Linear: model.layers.4.self_attn.k_proj -> 0.05:3b_64g/0.95:2b_64g s4, 2.16 bpw\n",
            " -- Linear: model.layers.4.self_attn.v_proj -> 0.05:3b_64g/0.95:2b_64g s4, 2.16 bpw\n",
            " -- Linear: model.layers.4.self_attn.o_proj -> 0.05:3b_64g/0.95:2b_64g s4, 2.13 bpw\n",
            " -- Module quantized, rfn_error: 0.010520\n",
            " -- Layer: model.layers.4 (MLP)\n",
            " -- Linear: model.layers.4.mlp.gate_proj -> 0.1:3b_64g/0.9:2b_64g s4, 2.17 bpw\n",
            " -- Linear: model.layers.4.mlp.up_proj -> 0.3:3b_64g/0.7:2b_64g s4, 2.38 bpw\n",
            " -- Linear: model.layers.4.mlp.down_proj -> 0.05:5b_32g/0.95:3b_32g s4, 3.24 bpw\n",
            " -- Module quantized, rfn_error: 0.016619\n",
            " -- Layer: model.layers.5 (Attention)\n",
            " -- Linear: model.layers.5.self_attn.q_proj -> 0.1:4b_128g/0.9:3b_128g s4, 3.16 bpw\n",
            " -- Linear: model.layers.5.self_attn.k_proj -> 0.1:4b_128g/0.9:3b_128g s4, 3.19 bpw\n",
            " -- Linear: model.layers.5.self_attn.v_proj -> 0.1:4b_64g/0.9:3b_64g s4, 3.20 bpw\n",
            " -- Linear: model.layers.5.self_attn.o_proj -> 0.1:4b_128g/0.9:3b_128g s4, 3.16 bpw\n",
            " -- Module quantized, rfn_error: 0.007419\n",
            " -- Layer: model.layers.5 (MLP)\n",
            " -- Linear: model.layers.5.mlp.gate_proj -> 0.1:4b_32g/0.9:3b_32g s4, 3.23 bpw\n",
            " -- Linear: model.layers.5.mlp.up_proj -> 0.25:4b_32g/0.75:3b_32g s4, 3.38 bpw\n",
            " -- Linear: model.layers.5.mlp.down_proj -> 0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4, 3.49 bpw\n",
            " -- Module quantized, rfn_error: 0.012564\n",
            " -- Layer: model.layers.6 (Attention)\n",
            " -- Linear: model.layers.6.self_attn.q_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Linear: model.layers.6.self_attn.k_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.6.self_attn.v_proj -> 1:5b_64g s4, 5.09 bpw\n",
            " -- Linear: model.layers.6.self_attn.o_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Module quantized, rfn_error: 0.003608\n",
            " -- Layer: model.layers.6 (MLP)\n",
            " -- Linear: model.layers.6.mlp.gate_proj -> 0.1:4b_32g/0.9:3b_32g s4, 3.23 bpw\n",
            " -- Linear: model.layers.6.mlp.up_proj -> 0.25:4b_32g/0.75:3b_32g s4, 3.38 bpw\n",
            " -- Linear: model.layers.6.mlp.down_proj -> 0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4, 3.49 bpw\n",
            " -- Module quantized, rfn_error: 0.014989\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.7 (Attention)\n",
            " -- Linear: model.layers.7.self_attn.q_proj -> 1:4b_128g s4, 4.04 bpw\n",
            " -- Linear: model.layers.7.self_attn.k_proj -> 1:4b_128g s4, 4.06 bpw\n",
            " -- Linear: model.layers.7.self_attn.v_proj -> 1:4b_128g s4, 4.06 bpw\n",
            " -- Linear: model.layers.7.self_attn.o_proj -> 1:4b_128g s4, 4.04 bpw\n",
            " -- Module quantized, rfn_error: 0.007035\n",
            " -- Layer: model.layers.7 (MLP)\n",
            " -- Linear: model.layers.7.mlp.gate_proj -> 0.1:4b_32g/0.9:3b_32g s4, 3.23 bpw\n",
            " -- Linear: model.layers.7.mlp.up_proj -> 0.25:4b_32g/0.75:3b_32g s4, 3.38 bpw\n",
            " -- Linear: model.layers.7.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.014504\n",
            " -- Layer: model.layers.8 (Attention)\n",
            " -- Linear: model.layers.8.self_attn.q_proj -> 1:4b_128g s4, 4.04 bpw\n",
            " -- Linear: model.layers.8.self_attn.k_proj -> 1:4b_128g s4, 4.06 bpw\n",
            " -- Linear: model.layers.8.self_attn.v_proj -> 1:4b_64g s4, 4.09 bpw\n",
            " -- Linear: model.layers.8.self_attn.o_proj -> 1:4b_128g s4, 4.04 bpw\n",
            " -- Module quantized, rfn_error: 0.007372\n",
            " -- Layer: model.layers.8 (MLP)\n",
            " -- Linear: model.layers.8.mlp.gate_proj -> 0.1:4b_32g/0.9:3b_32g s4, 3.23 bpw\n",
            " -- Linear: model.layers.8.mlp.up_proj -> 0.25:4b_32g/0.75:3b_32g s4, 3.38 bpw\n",
            " -- Linear: model.layers.8.mlp.down_proj -> 0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4, 3.49 bpw\n",
            " -- Module quantized, rfn_error: 0.018791\n",
            " -- Layer: model.layers.9 (Attention)\n",
            " -- Linear: model.layers.9.self_attn.q_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.16 bpw\n",
            " -- Linear: model.layers.9.self_attn.k_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.19 bpw\n",
            " -- Linear: model.layers.9.self_attn.v_proj -> 1:6b_128g s4, 6.06 bpw\n",
            " -- Linear: model.layers.9.self_attn.o_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.16 bpw\n",
            " -- Module quantized, rfn_error: 0.003062\n",
            " -- Layer: model.layers.9 (MLP)\n",
            " -- Linear: model.layers.9.mlp.gate_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.9.mlp.up_proj -> 0.25:5b_32g/0.75:4b_32g s4, 4.38 bpw\n",
            " -- Linear: model.layers.9.mlp.down_proj -> 0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4, 4.44 bpw\n",
            " -- Module quantized, rfn_error: 0.009912\n",
            " -- Layer: model.layers.10 (Attention)\n",
            " -- Linear: model.layers.10.self_attn.q_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Linear: model.layers.10.self_attn.k_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.10.self_attn.v_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.10.self_attn.o_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Module quantized, rfn_error: 0.007066\n",
            " -- Layer: model.layers.10 (MLP)\n",
            " -- Linear: model.layers.10.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.10.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.10.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.011176\n",
            " -- Layer: model.layers.11 (Attention)\n",
            " -- Linear: model.layers.11.self_attn.q_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.11.self_attn.k_proj -> 1:4b_32g s4, 4.16 bpw\n",
            " -- Linear: model.layers.11.self_attn.v_proj -> 1:4b_32g s4, 4.16 bpw\n",
            " -- Linear: model.layers.11.self_attn.o_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Module quantized, rfn_error: 0.010023\n",
            " -- Layer: model.layers.11 (MLP)\n",
            " -- Linear: model.layers.11.mlp.gate_proj -> 0.1:4b_32g/0.9:3b_32g s4, 3.23 bpw\n",
            " -- Linear: model.layers.11.mlp.up_proj -> 0.25:4b_32g/0.75:3b_32g s4, 3.38 bpw\n",
            " -- Linear: model.layers.11.mlp.down_proj -> 0.05:8b_32g/0.1:4b_32g/0.85:3b_32g s4, 3.49 bpw\n",
            " -- Module quantized, rfn_error: 0.022409\n",
            " -- Layer: model.layers.12 (Attention)\n",
            " -- Linear: model.layers.12.self_attn.q_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.16 bpw\n",
            " -- Linear: model.layers.12.self_attn.k_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.19 bpw\n",
            " -- Linear: model.layers.12.self_attn.v_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.12.self_attn.o_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.16 bpw\n",
            " -- Module quantized, rfn_error: 0.010207\n",
            " -- Layer: model.layers.12 (MLP)\n",
            " -- Linear: model.layers.12.mlp.gate_proj -> 1:4b_128g s4, 4.03 bpw\n",
            " -- Linear: model.layers.12.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.12.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.014418\n",
            " -- Layer: model.layers.13 (Attention)\n",
            " -- Linear: model.layers.13.self_attn.q_proj -> 0.1:6b_32g/0.9:5b_32g s4, 5.23 bpw\n",
            " -- Linear: model.layers.13.self_attn.k_proj -> 0.1:6b_32g/0.9:5b_32g s4, 5.26 bpw\n",
            " -- Linear: model.layers.13.self_attn.v_proj -> 1:6b_32g s4, 6.16 bpw\n",
            " -- Linear: model.layers.13.self_attn.o_proj -> 0.1:6b_32g/0.9:5b_32g s4, 5.23 bpw\n",
            " -- Module quantized, rfn_error: 0.004147\n",
            " -- Layer: model.layers.13 (MLP)\n",
            " -- Linear: model.layers.13.mlp.gate_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.16 bpw\n",
            " -- Linear: model.layers.13.mlp.up_proj -> 0.25:5b_128g/0.75:4b_128g s4, 4.28 bpw\n",
            " -- Linear: model.layers.13.mlp.down_proj -> 0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4, 4.36 bpw\n",
            " -- Module quantized, rfn_error: 0.015048\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.14 (Attention)\n",
            " -- Linear: model.layers.14.self_attn.q_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.14.self_attn.k_proj -> 1:4b_32g s4, 4.16 bpw\n",
            " -- Linear: model.layers.14.self_attn.v_proj -> 1:4b_32g s4, 4.16 bpw\n",
            " -- Linear: model.layers.14.self_attn.o_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Module quantized, rfn_error: 0.012923\n",
            " -- Layer: model.layers.14 (MLP)\n",
            " -- Linear: model.layers.14.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.14.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.14.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.015563\n",
            " -- Layer: model.layers.15 (Attention)\n",
            " -- Linear: model.layers.15.self_attn.q_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Linear: model.layers.15.self_attn.k_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.15.self_attn.v_proj -> 1:5b_64g s4, 5.09 bpw\n",
            " -- Linear: model.layers.15.self_attn.o_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Module quantized, rfn_error: 0.011713\n",
            " -- Layer: model.layers.15 (MLP)\n",
            " -- Linear: model.layers.15.mlp.gate_proj -> 1:4b_128g s4, 4.03 bpw\n",
            " -- Linear: model.layers.15.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.15.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.018786\n",
            " -- Layer: model.layers.16 (Attention)\n",
            " -- Linear: model.layers.16.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.16.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.16.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.16.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.010153\n",
            " -- Layer: model.layers.16 (MLP)\n",
            " -- Linear: model.layers.16.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.16.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.16.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.018980\n",
            " -- Layer: model.layers.17 (Attention)\n",
            " -- Linear: model.layers.17.self_attn.q_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Linear: model.layers.17.self_attn.k_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.17.self_attn.v_proj -> 1:5b_64g s4, 5.09 bpw\n",
            " -- Linear: model.layers.17.self_attn.o_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Module quantized, rfn_error: 0.011752\n",
            " -- Layer: model.layers.17 (MLP)\n",
            " -- Linear: model.layers.17.mlp.gate_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.16 bpw\n",
            " -- Linear: model.layers.17.mlp.up_proj -> 0.25:5b_128g/0.75:4b_128g s4, 4.28 bpw\n",
            " -- Linear: model.layers.17.mlp.down_proj -> 0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4, 4.36 bpw\n",
            " -- Module quantized, rfn_error: 0.021872\n",
            " -- Layer: model.layers.18 (Attention)\n",
            " -- Linear: model.layers.18.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.18.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.18.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.18.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.010848\n",
            " -- Layer: model.layers.18 (MLP)\n",
            " -- Linear: model.layers.18.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.18.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.18.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.022907\n",
            " -- Layer: model.layers.19 (Attention)\n",
            " -- Linear: model.layers.19.self_attn.q_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.16 bpw\n",
            " -- Linear: model.layers.19.self_attn.k_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.19 bpw\n",
            " -- Linear: model.layers.19.self_attn.v_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.19.self_attn.o_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.16 bpw\n",
            " -- Module quantized, rfn_error: 0.013906\n",
            " -- Layer: model.layers.19 (MLP)\n",
            " -- Linear: model.layers.19.mlp.gate_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.19.mlp.up_proj -> 0.25:5b_32g/0.75:4b_32g s4, 4.38 bpw\n",
            " -- Linear: model.layers.19.mlp.down_proj -> 0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4, 4.44 bpw\n",
            " -- Module quantized, rfn_error: 0.021642\n",
            " -- Layer: model.layers.20 (Attention)\n",
            " -- Linear: model.layers.20.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.20.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.20.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.20.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.009971\n",
            " -- Layer: model.layers.20 (MLP)\n",
            " -- Linear: model.layers.20.mlp.gate_proj -> 1:4b_128g s4, 4.03 bpw\n",
            " -- Linear: model.layers.20.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.20.mlp.down_proj -> 0.05:8b_32g/0.95:4b_128g s4, 4.25 bpw\n",
            " -- Module quantized, rfn_error: 0.026400\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.21 (Attention)\n",
            " -- Linear: model.layers.21.self_attn.q_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Linear: model.layers.21.self_attn.k_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.21.self_attn.v_proj -> 1:5b_64g s4, 5.09 bpw\n",
            " -- Linear: model.layers.21.self_attn.o_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Module quantized, rfn_error: 0.009716\n",
            " -- Layer: model.layers.21 (MLP)\n",
            " -- Linear: model.layers.21.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.21.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.21.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.023654\n",
            " -- Layer: model.layers.22 (Attention)\n",
            " -- Linear: model.layers.22.self_attn.q_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.16 bpw\n",
            " -- Linear: model.layers.22.self_attn.k_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.19 bpw\n",
            " -- Linear: model.layers.22.self_attn.v_proj -> 1:6b_128g s4, 6.06 bpw\n",
            " -- Linear: model.layers.22.self_attn.o_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.16 bpw\n",
            " -- Module quantized, rfn_error: 0.005309\n",
            " -- Layer: model.layers.22 (MLP)\n",
            " -- Linear: model.layers.22.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.22.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.22.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.023554\n",
            " -- Layer: model.layers.23 (Attention)\n",
            " -- Linear: model.layers.23.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.23.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.23.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.23.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.008197\n",
            " -- Layer: model.layers.23 (MLP)\n",
            " -- Linear: model.layers.23.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.23.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.23.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.023779\n",
            " -- Layer: model.layers.24 (Attention)\n",
            " -- Linear: model.layers.24.self_attn.q_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.16 bpw\n",
            " -- Linear: model.layers.24.self_attn.k_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.19 bpw\n",
            " -- Linear: model.layers.24.self_attn.v_proj -> 1:6b_128g s4, 6.06 bpw\n",
            " -- Linear: model.layers.24.self_attn.o_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.16 bpw\n",
            " -- Module quantized, rfn_error: 0.004919\n",
            " -- Layer: model.layers.24 (MLP)\n",
            " -- Linear: model.layers.24.mlp.gate_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.24.mlp.up_proj -> 0.25:5b_32g/0.75:4b_32g s4, 4.38 bpw\n",
            " -- Linear: model.layers.24.mlp.down_proj -> 0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4, 4.44 bpw\n",
            " -- Module quantized, rfn_error: 0.022033\n",
            " -- Layer: model.layers.25 (Attention)\n",
            " -- Linear: model.layers.25.self_attn.q_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Linear: model.layers.25.self_attn.k_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.25.self_attn.v_proj -> 1:5b_64g s4, 5.09 bpw\n",
            " -- Linear: model.layers.25.self_attn.o_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.18 bpw\n",
            " -- Module quantized, rfn_error: 0.008494\n",
            " -- Layer: model.layers.25 (MLP)\n",
            " -- Linear: model.layers.25.mlp.gate_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.25.mlp.up_proj -> 0.25:5b_32g/0.75:4b_32g s4, 4.38 bpw\n",
            " -- Linear: model.layers.25.mlp.down_proj -> 0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4, 4.44 bpw\n",
            " -- Module quantized, rfn_error: 0.022276\n",
            " -- Layer: model.layers.26 (Attention)\n",
            " -- Linear: model.layers.26.self_attn.q_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.16 bpw\n",
            " -- Linear: model.layers.26.self_attn.k_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.19 bpw\n",
            " -- Linear: model.layers.26.self_attn.v_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.26.self_attn.o_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.16 bpw\n",
            " -- Module quantized, rfn_error: 0.009997\n",
            " -- Layer: model.layers.26 (MLP)\n",
            " -- Linear: model.layers.26.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.26.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.26.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.024846\n",
            " -- Layer: model.layers.27 (Attention)\n",
            " -- Linear: model.layers.27.self_attn.q_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.16 bpw\n",
            " -- Linear: model.layers.27.self_attn.k_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.19 bpw\n",
            " -- Linear: model.layers.27.self_attn.v_proj -> 0.1:5b_64g/0.9:4b_64g s4, 4.20 bpw\n",
            " -- Linear: model.layers.27.self_attn.o_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.16 bpw\n",
            " -- Module quantized, rfn_error: 0.010132\n",
            " -- Layer: model.layers.27 (MLP)\n",
            " -- Linear: model.layers.27.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.27.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.27.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.025698\n",
            " -- Saving checkpoint...\n",
            " -- Layer: model.layers.28 (Attention)\n",
            " -- Linear: model.layers.28.self_attn.q_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.28.self_attn.k_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.26 bpw\n",
            " -- Linear: model.layers.28.self_attn.v_proj -> 1:5b_32g s4, 5.16 bpw\n",
            " -- Linear: model.layers.28.self_attn.o_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Module quantized, rfn_error: 0.009447\n",
            " -- Layer: model.layers.28 (MLP)\n",
            " -- Linear: model.layers.28.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.28.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.28.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.027911\n",
            " -- Layer: model.layers.29 (Attention)\n",
            " -- Linear: model.layers.29.self_attn.q_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.16 bpw\n",
            " -- Linear: model.layers.29.self_attn.k_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.19 bpw\n",
            " -- Linear: model.layers.29.self_attn.v_proj -> 1:6b_128g s4, 6.06 bpw\n",
            " -- Linear: model.layers.29.self_attn.o_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.16 bpw\n",
            " -- Module quantized, rfn_error: 0.007283\n",
            " -- Layer: model.layers.29 (MLP)\n",
            " -- Linear: model.layers.29.mlp.gate_proj -> 0.1:5b_32g/0.9:4b_32g s4, 4.23 bpw\n",
            " -- Linear: model.layers.29.mlp.up_proj -> 0.25:5b_32g/0.75:4b_32g s4, 4.38 bpw\n",
            " -- Linear: model.layers.29.mlp.down_proj -> 0.05:8b_32g/0.1:5b_32g/0.85:4b_32g s4, 4.44 bpw\n",
            " -- Module quantized, rfn_error: 0.026197\n",
            " -- Layer: model.layers.30 (Attention)\n",
            " -- Linear: model.layers.30.self_attn.q_proj -> 1:6b_128g s4, 6.04 bpw\n",
            " -- Linear: model.layers.30.self_attn.k_proj -> 1:6b_128g s4, 6.06 bpw\n",
            " -- Linear: model.layers.30.self_attn.v_proj -> 1:6b_128g s4, 6.06 bpw\n",
            " -- Linear: model.layers.30.self_attn.o_proj -> 1:6b_128g s4, 6.04 bpw\n",
            " -- Module quantized, rfn_error: 0.005726\n",
            " -- Layer: model.layers.30 (MLP)\n",
            " -- Linear: model.layers.30.mlp.gate_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.30.mlp.up_proj -> 1:4b_32g s4, 4.13 bpw\n",
            " -- Linear: model.layers.30.mlp.down_proj -> 0.05:8b_32g/0.95:4b_32g s4, 4.34 bpw\n",
            " -- Module quantized, rfn_error: 0.031432\n",
            " -- Layer: model.layers.31 (Attention)\n",
            " -- Linear: model.layers.31.self_attn.q_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.16 bpw\n",
            " -- Linear: model.layers.31.self_attn.k_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.19 bpw\n",
            " -- Linear: model.layers.31.self_attn.v_proj -> 1:6b_128g s4, 6.06 bpw\n",
            " -- Linear: model.layers.31.self_attn.o_proj -> 0.1:6b_128g/0.9:5b_128g s4, 5.16 bpw\n",
            " -- Module quantized, rfn_error: 0.007069\n",
            " -- Layer: model.layers.31 (MLP)\n",
            " -- Linear: model.layers.31.mlp.gate_proj -> 0.1:5b_128g/0.9:4b_128g s4, 4.16 bpw\n",
            " -- Linear: model.layers.31.mlp.up_proj -> 0.25:5b_128g/0.75:4b_128g s4, 4.28 bpw\n",
            " -- Linear: model.layers.31.mlp.down_proj -> 0.05:8b_32g/0.1:5b_128g/0.85:4b_128g s4, 4.36 bpw\n",
            " -- Module quantized, rfn_error: 0.032691\n",
            " -- Layer: model.norm (RMSNorm)\n",
            " -- Module quantized, rfn_error: 0.000000\n",
            " -- Layer: lm_head (Linear)\n",
            " -- Linear: lm_head -> 0.15:8b_128g/0.85:6b_128g s4, 6.34 bpw\n",
            " -- Module quantized, calibration perplexity (quant): 8.1155\n",
            " -- Saving checkpoint...\n",
            " -- Compiling output file...\n",
            " -- Writing shard 1...\n",
            " -- Creating directory Mistral-7B-Instruct-v0.3-exl2/4.0bpw/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model quantized and saved! You can test it with the following:"
      ],
      "metadata": {
        "id": "_4SN2kB4LaZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python exllamav2/test_inference.py -m {model_name}-exl2/{quant_bpw}bpw -p \"Once upon a time,\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NsMo-WYru70",
        "outputId": "a87ec6c2-662c-4d03-efdd-ad180557c4e8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -- Model: Mistral-7B-Instruct-v0.3-exl2/4.0bpw\n",
            " -- Options: []\n",
            " -- Loading model...\n",
            " -- Loaded model in 2.0031 seconds\n",
            " -- Loading tokenizer...\n",
            " -- Warmup...\n",
            " -- Generating...\n",
            "\n",
            "Once upon a time, in a land far, far away, children had to work hard to get their three meals a day. The chores were backbreaking and tedious. However, they were grateful for their food because they knew how difficult it was to produce, process, and prepare it.\n",
            "\n",
            "Fast forward to the present time: a century of technological advancement has made food readily available at our fingertips. We no longer have to toil in the fields to gather food, or spend hours in the kitchen to prepare it. The ease with which we can now access delicious food has led to the proliferation of fast food chains\n",
            "\n",
            " -- Response generated in 2.27 seconds, 128 tokens, 56.51 tokens/second (includes prompt eval.)\n"
          ]
        }
      ]
    }
  ]
}
